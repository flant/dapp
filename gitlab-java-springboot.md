---
author_team: "charlie"
author_name: "Евгений Ермонин"
ci: "gitlab"
language: "java"
framework: "springboot"
is_compiled: 1
package_managers_possible:
 - maven
 - gradle
package_managers_chosen: "junit"
unit_tests_possible:
 - junit
 - testng
unit_tests_chosen: "flask-sqlalchemy"
assets_generator_possible:
 - webpack
assets_generator_chosen: "webpack"
---

# Чек-лист готовности статьи
<ol>
<li>Все примеры кладём в <a href="https://github.com/flant/examples">https://github.com/flant/examples</a>

<li>Для каждой статьи может и должно быть НЕСКОЛЬКО примеров, условно говоря — по примеру на главу это нормально.

<li>Делаем примеры И на Dockerfile, И на Stapel

<li>Про хельм говорим, про особенности говорим, но в подробности не вдаёмся — считаем, что человек умеет в кубовые ямлы.

<li>Обязательно тестируйте свои примеры перед публикацией
</li>
</ol>

# Введение

Рассмотрим разные способы которые помогут собрать Java-приложение на примере springboot и запустить его в kubernetes кластере.

Предполагается что читатель имеет базовые знания в разработке на Java а также немного знаком с Gitlab CI и примитивами kubernetes, либо готов во всём этом разобраться самостоятельно. Мы постараемся предоставить все ссылки на необходимые ресурсы, если потребуется приобрести какие то новые знания.  

Собирать приложения будем с помощью werf. Данный инструмент работает в Linux MacOS и Windows, инструкция по [установке](https://ru.werf.io/documentation/guides/installation.html) находится на официальном [сайте](https://ru.werf.io/). В качестве примера - также приложим Docker файлы.

Для иллюстрации действий в данной статье - создан репозиторий с исходным кодом, в котором находятся несколько простых приложений. Мы постараемся подготовить примеры чтобы они запускались на вашем стенде и постараемся подсказать, как отлаживать возможные проблемы при вашей самостоятельной работе.


## Подготовка приложения

Наилучшим образом приложения будут работать в Kubernetes - если они соответствуют [12 факторам heroku](https://12factor.net/). Благодаря этому - у нас в kubernetes работают stateless приложения, которые не зависят от среды. Это важно, так как кластер может самостоятельно переносить приложения с одного узла на другой, заниматься масштабированием и т.п. — и мы не указываем, где конкретно запускать приложение, а лишь формируем правила, на основании которого кластер принимает свои собственные решения.

Договоримся что наши приложения соответствуют этим требованиям. На хабре уже было описание данного подхода, вы можете почитать про него например [тут](https://12factor.net/).


## Подготовка и настройка среды

Для того, чтобы пройти по этому гайду, необходимо, чтобы

*   У вас был работающий и настроенный Kubernetes кластер
*   Код приложения находился в Gitlab
*   Был настроен Gitlab CI, подняты и подключены к нему раннеры

Для пользователя под которым будет производиться запуск runner-а - нужно установить multiwerf - данная утилита позволяет переключаться между версиями werf и автоматически обновлять его. Инструкция по установке - доступна по [ссылке](https://ru.werf.io/documentation/guides/installation.html#installing-multiwerf).

Для автоматического выбора актуальной версии werf в канале stable, релиз 1.1 выполним следующую  команду:

```
. $(multiwerf use 1.1 stable --as-file)
```

Перед деплоем нашего приложения необходимо убедиться что у нас подготовлены инфраструктурные компоненты:

*   К gitlab подключен shell runner с тегом werf. [Инструкция](https://ru.werf.io/documentation/guides/gitlab_ci_cd_integration.html#%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-runner) по подготовке gitlab runner.
*   Ранеры включены и активны для репозитория с нашим приложением
*   Для пользователя под которым запускается сборка и деплой установлен kubectl и добавлен конфигурационный файл для подключения к kubernetes.
*   Для gitlab включен и настроен gitlab registry
*   Раннер запущен на отдельной виртуалке, имеет доступ к API kubernetes и запускается по тегу werf

## Настройка Gitlab Runner

{{TODO: откорректировать то, что ниже}}

Теперь обязательно на сервере с gitlab-runner, который занимается сборкой вашего приложения, установить [kubectl](https://kubernetes.io/ru/docs/tasks/tools/install-kubectl/). И положить в домашнюю директорию пользователя gitlab-runner конфиг kubernetes в папку `.kube` (нужно её создать, если её нет)

Конфиг можно взять на мастере кластера kubernetes в `/etc/kubernetes/admin.conf`

Скопируйте его и положите в папку `.kube` переименовав файл в `config`.

![alt_text](images/-5.png "image_tooltip")

Обычно в конфиге указан прямой адрес мастера kubernetes. Соответственно нужно чтобы мастер нашего кластера был доступен по сети для gitlab-runner.

Всё это мы сделали для того чтобы werf мог общаться с API kubernetes и деплоить в него наши приложения, это и есть второй ответ на вопрос “Как werf понимает куда ему нужно деплоить?”. По умолчанию деплой будет происходить в namespace состоящий из имени проекта задаваемого в `werf.yaml` и окружения задаваемого в `.gitlab-ci.yml` куда мы деплоим наше приложение.
# Hello world

В первой главе мы покажем поэтапную сборку и деплой приложения без задействования внешних ресурсов таких как база данных и сборку ассетов.

Наше приложение будет состоять из одного docker образа собранного с помощью werf. Его единственной задачей будет вывод сообщения “hello world” по http.

В нашем случае будет работать процесс java, исполняющий собранный jar отдающий hello world по http.

Управлять маршрутизацией запросов к приложению будет управлять Ingress в kubernetes кластере.

Мы реализуем два стенда: production и staging. В рамках hello world приложения мы предполагаем, что разработка ведётся локально, на вашем компьютере.

_В ближайшее время werf реализует удобные инструменты для локальной разработки, следите за обновлениями._


## Локальная сборка

Поскольку собирать мы будем spring-фреймворк - для скачивания шаблона приложения перейдем на start.spring.io. Оставляем все поля как есть, справа добавляем в dependencies только "Spring Web" и нажмем generate. Разархивируем полученный архив - получим готовую структуру папок и нужные нам файлы для того чтобы описать простейшее приложение.
tree:

```
├── HELP.md
├── mvnw
├── mvnw.cmd
├── pom.xml
└── src
    ├── main
    │   ├── java
    │   │   └── com
    │   │       └── example
    │   │           └── demo
    │   │               └── DemoApplication.java
    │   └── resources
    │       └── application.properties
    └── test
        └── java
            └── com
                └── example
                    └── demo
                        └── DemoApplicationTests.java

12 directories, 7 files
```

pom.xml у нас сгенерирован автоматически, там  правки не нужны.
application.properties на данном этапе так же оставим пустыми
А DemoApplication чуть поправим чтобы приложение по http отвечало Hello World:

```java
package com.example.demo;
  
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;


@SpringBootApplication
@RestController
public class DemoApplication {

        @RequestMapping("/")
        public String home() {
                return "Hello World";
        }


        public static void main(String[] args) {
                SpringApplication.run(DemoApplication.class, args);
        }

}
```

И инициализируем здесь git-репозиторий (чуть ниже будет рассказано зачем)

```bash
git init
git add .
git commit -m 'initial commit'
```

Для того чтобы werf смогла начать работу с нашим приложением - необходимо в корне нашего репозитория создать файл werf.yaml в которым будут описаны инструкции по сборке. Для начала соберем образ локально не загружая его в registry чтобы разобраться с синтаксисом сборки.

С помощью werf можно собирать образы с используя Dockerfile или используя синтаксис, описанный в документации werf (мы называем этот синтаксис и движок, который этот синтаксис обрабатывает, stapel). Для лучшего погружения - соберем наш образ с помощью stapel.

Итак, начнём с самой главной секции нашего werf.yaml файла, которая должна присутствовать в нём **всегда**. Называется она [meta config section](https://werf.io/documentation/configuration/introduction.html#meta-config-section) и содержит всего два параметра.

werf.yaml:
```yaml
project: spring
configVersion: 1
```

**_project_** - поле, задающее имя для проекта, которым мы определяем связь всех docker images собираемых в данном проекте. Данное имя по умолчанию используется в имени helm релиза и имени namespace в которое будет выкатываться наше приложение. Данное имя не рекомендуется изменять (или подходить к таким изменениям с должным уровнем ответственности) так как после изменений уже имеющиеся ресурсы, которые выкачаны в кластер, не будут переименованы.

**_configVersion_** - в данном случае определяет версию синтаксиса используемую в `werf.yaml`.

После мы сразу переходим к следующей секции конфигурации, которая и будет для нас основной секцией для сборки - [image config section](https://werf.io/documentation/configuration/introduction.html#image-config-section). И чтобы werf понял что мы к ней перешли разделяем секции с помощью тройной черты.


```yaml
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
```

**_image_** задает короткое имя собираемого docker-образа. Должно быть уникально в рамках одного werf-файла.

**_from _** - аналогичная секция с обычным dockerfile. В примере spring используется `openjdk:8-jdk-alpine, `но он хорош для запуска, мы же воспользуемся образом в котором уже предустановлены все что необходимо maven для сборки - `maven:3-jdk-8.`

Теперь встает вопрос о том как нам добавить исходный код приложения внутрь нашего docker image. И для этого мы можем использовать Git! И нам даже не придётся устанавливать его внутрь docker image.

**_git_**, на наш взгляд это самый правильный способ добавления ваших исходников внутрь docker image, хотя существуют и другие. Его преимущество в том что он именно клонирует, и в дальнейшем накатывает коммитами изменения в тот исходный код что мы добавили внутрь нашего docker image, а не просто копирует файлы. Вскоре мы узнаем зачем это нужно.

```yaml
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
git:
- add: /
  to: /app
```

Werf подразумевает что ваша сборка будет происходить внутри директории склонированного git репозитория. Потому мы списком можем указывать директории и файлы относительно корня репозитория которые нам нужно добавить внутрь image.

`add: /` - та директория которую мы хотим добавить внутрь docker image, мы указываем, что это весь наш репозиторий

`to: /app` - то куда мы клонируем наш репозиторий внутри docker image. Важно заметить что директорию назначения werf создаст сам.

 Есть возможность даже добавлять внешние репозитории внутрь проекта не прибегая к предварительному клонированию, как это сделать можно узнать [тут](https://werf.io/documentation/configuration/stapel_image/git_directive.html), но мы не рекомендуем такой подход.

Далее описывается используемыс сборщик а так же стадии сборки.
Сейчас доступно 2 вида сборщика - shell и ansible. Первый аналогичен директиве RUN в dockerfile. Его удобнее использовать для быстрого получения результата с минимальными затратами времени на изучение. ansible более молодой инструмент и требующий несколько большего времени на изучение, но он позволяет получить более прогнозируемый результат вследствии декларативности. 
Пользовательские стадии - их всего 4 before install, install, before setup, setup и детально мы к ним вернемся в разделе управления зависимостями. Подробнее о них можно почитать в [документации](https://werf.io/documentation/configuration/stapel_image/assembly_instructions.html#usage-of-user-stages)

Соберем jar
Однако, чтобы запускать jar его нужно предварительно собрать. Предлагается сделать это локально, мы же соберем jar так же используя werf и ansible-сборшик. Поскольку все системные зависимости для сборки удовлетворены - мы используем образ openjdk с maven - опишем сборку приложения в стадии setup:

```yaml
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
git:
- add: /
  to: /app
ansible:
  setup:
  - name: Build jar
    shell: |
      mvn -B -f pom.xml package dependency:resolve
    args:
      chdir: /app
      executable: /bin/bash
```

Уже сейчас можем запустить сборку и получить docker-образ с лежащим внутри jar.

Полный список поддерживаемых модулей ansible в werf можно найти [тут](https://werf.io/documentation/configuration/stapel_image/assembly_instructions.html#supported-modules).

Не забыв [установить werf](https://werf.io/documentation/guides/installation.html) локально, запускаем сборку с помощью [werf build](https://werf.io/documentation/cli/main/build.html)!

```bash
$  werf build --stages-storage :local
```

TODO: картинка? \

![alt_text](images/-werf0.gif "image_tooltip")

Вот и всё, наша сборка успешно завершилась. К слову если сборка падает и вы хотите изнутри контейнера её подебажить вручную, то вы можете добавить в команду сборки флаги:

```yaml
--introspect-before-error
```

или

```yaml
--introspect-error
```

Которые при падении сборки на одном из шагов автоматически откроют вам shell в контейнер, перед исполнением проблемной инструкции или после.

В конце werf отдал информацию о готовом image:

![alt_text](images/-1.png "image_tooltip")

Теперь его можно запустить локально используя image_id просто с помощью docker.
Либо вместо этого использовать [werf run](https://werf.io/documentation/cli/main/run.html):


```
werf run --stages-storage :local --docker-options="-d -p 8080:8080 --restart=always" -- java -jar /app/target/demoApplication.jar
```

Первая часть команды очень похожа на build, а во второй мы задаем [параметры](https://docs.docker.com/engine/reference/run/) docker и через двойную черту команду с которой хотим запустить наш image.

Небольшое пояснение про `--stages-storage :local `который мы использовали и при сборке и при запуске приложения. Данный параметр указывает на то где werf хранить стадии сборки. На момент написания статьи это возможно только локально, но в ближайшее время появится возможность сохранять их в registry.

Теперь наше приложение доступно локально на порту 8080:

![alt_text](images/-2.png "image_tooltip")

На этом часть с локальным использованием werf мы завершаем и переходим к той части для которой werf создавался, использовании его в CI.

## Построение CI-процесса

После того как мы закончили со сборкой, которую можно производить локально, мы приступаем к базовой настройке CI/CD на базе Gitlab.

Начнем с того что добавим нашу сборку в CI с помощью .gitlab-ci.yml, который находится внутри корня проекта. Нюансы настройки CI в Gitlab можно найти [тут](https://docs.gitlab.com/ee/ci/).

Мы предлагаем простой флоу, который мы называем [fast and furious](https://docs.google.com/document/d/1a8VgQXQ6v7Ht6EJYwV2l4ozyMhy9TaytaQuA9Pt2AbI/edit#). Такой флоу позволит вам осуществлять быструю доставку ваших изменений в production согласно методологии GitOps и будут содержать два окружения, production и stage.

На стадии сборки мы будем собирать образ с помощью werf и загружать образ в registry, а затем на стадии деплоя собрать инструкции для kubernetes, чтобы он скачивал нужные образы и запускал их.

### Сборка в Gitlab CI

Для того, чтобы настроить CI-процесс создадим .gitlab-ci.yaml в корне репозитория.

Инициализируем werf перед запуском основной команды. Это необходимо делать перед каждым использованием werf поэтому мы вынесли в секцию `before_script`
Такой сложный путь с использованием multiwerf нужен для того, чтобы вам не надо было думать про обновление верфи и установке новых версий — вы просто указываете, что используете, например, use 1.1 stable и пребываете в уверенности, что у вас актуальная версия с закрытыми issues.

```yaml
before_script:
  - type multiwerf && source <(multiwerf use 1.1 stable)
  - type werf && source <(werf ci-env gitlab --verbose)
```

`werf ci-env gitlab --verbose` - готовит наш werf для работы в Gitlab, выставляя для этого все необходимые переменные.
Пример переменных автоматически выставляемых этой командой:

```bash
### DOCKER CONFIG
 export DOCKER_CONFIG="/tmp/werf-docker-config-832705503"
 ### STAGES_STORAGE
 export WERF_STAGES_STORAGE="registry.gitlab-example.com/chat/stages"
 ### IMAGES REPO
 export WERF_IMAGES_REPO="registry.gitlab-example.com/chat"
 export WERF_IMAGES_REPO_IMPLEMENTATION="gitlab"
 ### TAGGING
 export WERF_TAG_BY_STAGES_SIGNATURE="true"
 ### DEPLOY
 # export WERF_ENV=""
 export WERF_ADD_ANNOTATION_PROJECT_GIT="project.werf.io/git=https://lab.gitlab-example.com/chat"
 export WERF_ADD_ANNOTATION_CI_COMMIT="ci.werf.io/commit=61368705db8652555bd96e68aadfd2ac423ba263"
 export WERF_ADD_ANNOTATION_GITLAB_CI_PIPELINE_URL="gitlab.ci.werf.io/pipeline-url=https://lab.gitlab-example.com/chat/pipelines/71340"
 export WERF_ADD_ANNOTATION_GITLAB_CI_JOB_URL="gitlab.ci.werf.io/job-url=https://lab.gitlab-example.com/chat/-/jobs/184837"
 ### IMAGE CLEANUP POLICIES
 export WERF_GIT_TAG_STRATEGY_LIMIT="10"
 export WERF_GIT_TAG_STRATEGY_EXPIRY_DAYS="30"
 export WERF_GIT_COMMIT_STRATEGY_LIMIT="50"
 export WERF_GIT_COMMIT_STRATEGY_EXPIRY_DAYS="30"
 export WERF_STAGES_SIGNATURE_STRATEGY_LIMIT="-1"
 export WERF_STAGES_SIGNATURE_STRATEGY_EXPIRY_DAYS="-1"
 ### OTHER
 export WERF_LOG_COLOR_MODE="on"
 export WERF_LOG_PROJECT_DIR="1"
 export WERF_ENABLE_PROCESS_EXTERMINATOR="1"
 export WERF_LOG_TERMINAL_WIDTH="95"
```


Многие из этих переменных интуитивно понятны, и содержат базовую информацию о том где находится проект, где находится его registry, информацию о коммитах. \
Подробную информацию о конфигурации ci-env можно найти [тут](https://werf.io/documentation/reference/plugging_into_cicd/overview.html). От себя лишь хочется добавить, что если вы используете совместно с Gitlab внешний registry (harbor,Docker Registry,Quay etc.), то в команду билда и пуша нужно добавлять его полный адрес (включая путь внутри registry), как это сделать можно узнать [тут](https://werf.io/documentation/cli/main/build_and_publish.html). И так же не забыть первой командой выполнить [docker login](https://docs.docker.com/engine/reference/commandline/login/).

В рамках статьи нам хватит значений выставляемых по умолчанию.

Переменная [WERF_STAGES_STORAGE](https://ru.werf.io/documentation/reference/stages_and_images.html#%D1%85%D1%80%D0%B0%D0%BD%D0%B8%D0%BB%D0%B8%D1%89%D0%B5-%D1%81%D1%82%D0%B0%D0%B4%D0%B8%D0%B9) указывает где werf сохраняет свой кэш (стадии сборки) У werf есть опция распределенной сборки, про которую вы можете прочитать в нашей статье, в текущем примере мы сделаем по-простому и сделаем сборку на одном узле в один момент времени.

```yaml
variables:
    WERF_STAGES_STORAGE: ":local"
```
Дело в том что werf хранит стадии сборки раздельно, как раз для того чтобы мы могли не пересобирать весь образ, а только отдельные его части.

Плюс стадий в том, что они имеют собственный тэг, который представляет собой хэш содержимого нашего образа. Тем самым позволяя полностью избегать не нужных пересборок наших имиджей. Если вы собираете ваше приложение в разных ветках, и исходный код в них различается только конфигами которые используются для генерации статики на последней стадии. То при сборке имиджа одинаковые стадии пересобираться не будут, будут использованы уже собранные стадии из соседней ветки. Тем самым мы резко снижаем время доставки кода.

Основная команда на текущий момент - это werf build-and-publish, которая запускает сборку и публикацию в registry на gitlab runner с тегом werf для любой ветки. Путь до registry и другие параметры беруться верфью автоматически их переменных окружения gitlab ci.

```yaml
Build:
  stage: build
  script:
    - werf build-and-publish
  tags:
    - werf
```

Если вы всё правильно сделали и корректно настроен registry и gitlab ci — вы увидите собранный образ в registry. При использовании registry от gitlab — собранный образ можно увидеть через веб-интерфейс гитлаба.

Следующие параметры тем кто работал с гитлаб уже должны быть знакомы.

**_tags_** - нужен для того чтобы выбрать наш раннер, на который мы навесили этот тэг. В данном случае наш gitlab-runner в Gitlab имеет тэг werf

```yaml
  tags:
    - werf
```


{{Если есть компиляция — описываем как её делать}}

{{Отдельно проговариваем историю с проброской конфигов в стадию сборки.}} 

Теперь мы можем запушить наши изменения и увидеть что наша стадия успешно выполнилась.

![alt_text](images/-3.png "image_tooltip")


Лог в Gitlab будет выглядеть так же как и при локальной сборке, за исключением того что в конце мы увидим как werf пушит наш docker image в registry.

```
207 │ ┌ Publishing image {{node}} by stages-signature tag c905b748cb9647a03476893941837bf79910ab09e ...
208 │ ├ Info
209 │ │   images-repo: registry.gitlab-example.com/{{chat/node}}
210 │ │        image: registry.gitlab-example.com/{{chat/node}}:c905b748cb9647a03476893941 ↵
211 │ │   837bf79910ab09ef5878037592a45d
212 │ └ Publishing image {{node}} by stages-signature tag c905b748cb9647a0347689394 ... (14.90 seconds)
213 └ ⛵ image {{node}} (73.44 seconds)
214 Running time 73.47 seconds
218 Job succeeded
```

### Деплой в Kubernetes

Werf использует встроенный Helm для применения конфигурации в Kubernetes. Для описания объектов Kubernetes werf использует конфигурационные файлы Helm: шаблоны и файлы с параметрами (например, values.yaml). Помимо этого, werf поддерживает дополнительные файлы, такие как файлы c секретами и с секретными значениями (например secret-values.yaml), а также дополнительные Go-шаблоны для интеграции собранных образов.

Werf (по аналогии с helm) берет yaml шаблоны, которые описывают объекты Kubernetes, и генерирует из них общий манифест. Манифест отдается API Kubernetes, который на его основе внесет все необходимые изменения в кластер. Werf отслеживает как Kubernetes вносит изменения и сигнализирует о результатах в реальном времени. Все это благодаря встроенной в werf библиотеке [kubedog](https://github.com/flant/kubedog).

Внутри Werf доступны команды для работы с Helm, например можно проверить как сгенерируется общий манифест в результате работы werf с шаблонами:

```bash
$ werf helm render
```

Аналогично, доступны команды [helm list](https://werf.io/documentation/cli/management/helm/list.html) и другие.

#### Общее про хельм-конфиги

На сегодняшний день [Helm](https://helm.sh/) один из самых удобных способов которым вы можете описать свой deploy в Kubernetes. Кроме возможности установки готовых чартов с приложениями прямиком из репозитория, где вы можете введя одну команду, развернуть себе готовый Redis, Postgres, Rabbitmq прямиком в Kubernetes, вы также можете использовать Helm для разработки собственных чартов с удобным синтаксисом для шаблонизации выката ваших приложений.

Потому для werf это был очевидный выбор использовать такую технологию.

Мы не будем вдаваться в подробности разработки yaml манифестов с помощью Helm для Kubernetes. Осветим лишь отдельные её части, которые касаются данного приложения и werf в целом. Если у вас есть вопросы о том как именно описываются объекты Kubernetes, советуем посетить страницы документации по Kubernetes с его [концептами](https://kubernetes.io/ru/docs/concepts/) и страницы документации по разработке [шаблонов](https://helm.sh/docs/chart_template_guide/) в Helm.

Нам понадобятся следующие файлы со структурой каталогов:


```
.helm (здесь мы будем описывать деплой)
├── templates (объекты kubernetes в виде шаблонов)
│   ├── deployment.yaml (основное приложение)
│   ├── ingress.yaml (описание для ingress)
│   └── service.yaml (сервис для приложения)
├── secret-values.yaml (файл с секретными переменными)
└── values.yaml (файл с переменными для параметризации шаблонов)
```

Подробнее читайте в [нашей статье](https://habr.com/ru/company/flant/blog/423239/) из серии про Helm.

![alt_text](images/-4.png "image_tooltip")

#### Описание приложения в хельме

Для работы нашего приложения в среде Kubernetes понадобится описать сущности Deployment, Service и завернуть трафик на приложение, донастроив роутинг в кластере с помощью сущности Ingress.

{{TODO: вот это единое в шаблоне надо привести в порядок}}

Не забываем создать валидный ключ для доступа из kubernetes к registry gitlab.

```yaml
      imagePullSecrets:
      - name: registrysecret
```

##### Запуск контейнера

Хуё моё для запука контейнера надо какой-то процесс прописывать и указать в каком контейнере это будет. Хуё-моё образ-то мог поменяться при пересборке, а мог нет. И хуё-моё тырыцпыры

Верфь умеет делать чуток магии

Ëжики пирожики вот прописываем запуск контейнера

       command:
       - java
       - -jar
       - /app/demo.jar $JAVA_OPT

Ежики пирожики про werf_container image.

Там будет подставлен ёжики мирожики на основаии контент бейз теггинг. И это очень хорошо.






Приступим к описанию приложения в helm. Опишем запуск самого приложение в deployment.yaml:

deployment.yaml
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: {{ .Chart.Name | quote }}
spec:
 replicas: {{ pluck .Values.global.env .Values.app.replicas | first | default .Values.app.replicas._default }}
 selector:
   matchLabels:
     app: {{ .Chart.Name | quote }}
 template:
   metadata:
     labels:
       app: {{ .Chart.Name | quote }}
   spec:
     containers:
     - name: {{ .Chart.Name | quote }}
{{ tuple "hello" . | include "werf_container_image" | indent 8 }}
       command:
       - java
       - -jar
       - /app/demo.jar $JAVA_OPT
       workingDir: /app
       ports:
       - name: http
         containerPort: 8080
         protocol: TCP
       env:
{{ tuple "hello" . | include "werf_container_env" | indent 8 }}
```

Особенности шаблона, которые появляются при использовании werf:

* {{ . Chart.Name }} - это тот самый project и werf.yaml
* {{ pluck .Values.global.env .Values.app.replicas | first | default .Values.app.replicas._default }} - в этой строчке берется global.env которую мы передадим во время деплоя приложения ниже, и в зависимости от окружения применится либо значение для этого окружения (например, production, либо то что прописано в _default в values.yaml - чуть подробнее об этом файле в следующей главе)
* {{ tuple "hello" . | include "werf_container_image" | indent 8 }} - добавляет информацию о собранном werf-ем image. То есть откуда его нужно стянуть (registry), имя образа и тег. А так же информацию о политике скачивания образа - IfNotPresent.
* {{ tuple "hello" . | include "werf_container_env" | indent 8 }} - [документация](https://ru.werf.io/documentation/reference/deploy_process/deploy_into_kubernetes.html#werf_container_env), в следующей главе опишем подробнее.

В остальном же это обычный шаблон, описание параметров которого можно найти в официальной документации по kubernetes.

##### Переменные окружения

Для корректной работы нашего приложения ему нужно узнать переменные окружения.
Для Java это, например, данные о подключении к БД - хост, юзер, пароль - чтобы не "хардкодить" эти данные внутрь приложения. Так же мы можем изменить команду запуска, в зависимости от переменной окружения, которую передадим в команду запуска приложения. Это позволит нам используя один и тот же собранные docker-образ с одним и тем же кодом запускать в разных окружениях - с разными подключениям к БД, с разными лимитами на процесс java, с разным режимом дебага (в нашем примере это JAVA_OPT).

И эти переменные можно параметризовать с помощью файла `values.yaml`.

Например вот так мы пробросим количество реплик, с которыми должно запуститься приложение, опции для старта java-приложения уже в kubernetes и, на будущее, определим домены по которым будет доступно приложение после деплоя:

```yaml
---
app:
  replicas:
    _default: 1
    production: 1
  java_opt:
    _default: "--debug"
    production: ""
  url:
    _default: stage.example.com
    prodiction: example.com
```

Переменные окружения иногда используются для того, чтобы не перевыкатывать контейнеры, которые не менялись.

Werf закрывает ряд вопросов, связанных с перевыкатом контейнеров с помощью конструкции  [werf_container_env](https://ru.werf.io/documentation/reference/deploy_process/deploy_into_kubernetes.html#werf_container_env). Она возвращает блок с переменной окружения DOCKER_IMAGE_ID контейнера пода. Значение переменной будет установлено только если .Values.global.werf.is_branch=true, т.к. в этом случае Docker-образ для соответствующего имени и тега может быть обновлен, а имя и тег останутся неизменными. Значение переменной DOCKER_IMAGE_ID содержит новый ID Docker-образа, что вынуждает Kubernetes обновить объект.

```yaml
{{ tuple "rails" . | include "werf_container_env" | indent 8 }}
```

Аналогично можно пробросить секретные переменные (пароли и т.п.) и у Верфи есть специальный механизм для этого. Но к этому вопросу мы вернёмся позже.

##### Логгирование

Важно понимать, что все логи какие возможно приложение должно писать в stdout. Запись логов в файл будет как менее удобна в диагностике возможных проблем так и сама по себе может привести к проблеме с заканчивающимся местом на ноде kubernetes. А это уже приведет к автоматической попытке kubernetes расчистить место - под переедет на другую ноду, в результате получим рестарт пода и потерю всех логов.
Так же из stdout (посредством лога на ноде kubernetes) мы можем забирать централизованно их в аггрегатор логов, например fluent-ом. Это потребует меньше настроек, так как stdout-логи подов лежат в одном месте для всех подов.
В случае с springboot приложение уже настроено на логирование в stdout по умолчанию.


##### Направление трафика на приложение

Для того чтобы запросы извне попали к нам в приложение нужно открыть порт у пода, привязать к поду сервис и настроить Ingress, который выступает у нас в качестве балансера.

Если вы мало работали с Kubernetes — эта часть может вызвать у вас много проблем. Большинство тех, кто начинает работать с Kubernetes по невнимательности допускают ошибки при конфигурировании labels и затем занимаются долгой и мучительной отладкой.

{{TODO: тут бы дать какую-то подсказку, как человеку пройти через это и не поседеть, если у него рядом нет ментора, который ткнёт ему в опечатку}}

###### Прокрутить порт

Для того чтобы попадать в наше приложение в kubernetes есть объект [service](https://kubernetes.io/docs/concepts/services-networking/service/).

Нам нужно бубет ёжики пирожики сделать сервис и связать его с помощью лейблов фиг туырыпак. Тогдща фигак тырыпак запрос будет приходить на сервис, тот пробросит туда-то на такой-то прокрученный порт.

Связь между service и подом в deployment описана в блоке spec. Selector описывает по каким label нужно искать нужный под - подобная секция описана и в deployment - там присваивается этот label:

```yaml
 template:
   metadata:
     labels:
       app: {{ .Chart.Name | quote }}
```

Объект service позволяет сопоставлять любой входящий port и targetPort, например входящий 80 и порт в контейнере 8080, в нашем случае они одинаковы и указаны оба для прозрачности. Спецификация же позволяет не указывать targetPort если он совпадает с port. Можно так же указать несколько портов, если это требуется приложению. Для простоты оставим один порт.

###### Роутинг на Ingress

Мы описали сервис и теперь можем направить внешний (по отношению к кластеру kubernetes) трафик в приложение. Для этого опишем [ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) - направим все запросы на корень домена в service описанный выше на порт 8080.

Для этого нужно будет указать набор правил роутинга: с какого домена, с какого пути, в какой сервис и на какой порт надо направлять трафик.

Домен будем задавать так
                       
```yaml
  - host: {{ pluck .Values.global.env .Values.app.url | first | default .Values.app.url._default }}
```

тыры пыры здесь написнао то-то для такой-то цели

Для каждого домена могут существовать набор правил для разных путей. В нашем примере
                                                                        
```yaml
      - path: /
        backend:
          serviceName: {{ .Chart.Name | quote }}
          servicePort: 8080
```

тыры пыры восемь дыр


```yaml
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: {{ .Chart.Name | quote }}
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: {{ pluck .Values.global.env .Values.app.url | first | default .Values.app.url._default }}
    http:
      paths:
      - path: /
        backend:
          serviceName: {{ .Chart.Name | quote }}
          servicePort: 8080
```

Здесь встречается похожая на используюмую ранее конструкция

```yaml
- host: {{ pluck .Values.global.env .Values.app.url | first | default .Values.app.url._default }}
```

Аналогично тому что использовали ранее - выбираем какой использовать домен в данном окружении. Либо явно объявленный для этого конкретного оркужения (production, к примеру) либо берем домен из строчки _default.


#### Секретные переменные

Мы уже рассказывали о том как использовать обычные переменные в нашем СI забирая их напрямую из values.yaml. Суть работы с секретными переменными абсолютно та же, единственное что в репозитории они будут храниться в зашифрованном виде.

Потому для хранения в репозитории паролей, файлов сертификатов и т.п., рекомендуется использовать подсистему работы с секретами werf.

Идея заключается в том, что конфиденциальные данные должны храниться в репозитории вместе с приложением в зашифрованном виде, и должны оставаться независимыми от какого-либо конкретного сервера.

Для того, чтобы начать пользоваться секретными переменными нужно сгенерировать секретный ключ с которым werf сможет его шифровать и расшифровывать во время деплоя.
Делается это из консоли:

```bash
$ werf helm secret generate-secret-key
4710f841e17fabcc85f976e4a665ff9e
```

Чтобы им воспользоваться нужно добавить его либо в свои переменные окружения, либо записать в файл .werf_secret_key. Во втором случае нужно обязательно добавить его в gitignore.

```bash
export WERF_SECRET_KEY=4710f841e17fabcc85f976e4a665ff9e
echo $WERF_SECRET_KEY > .werf_secret_key
```

Теперь добавим секретную перменную, например, password:

```bash
werf helm secret values edit .helm/secret-values.yaml
```

Откроется редактор по умолчанию, куда мы впишем наши парои plain-text в формате values.yaml. Например:

```yaml
app:
  password:
    _default: my-secret-password
    production: my-super-secret-password
```

При просмотре результирующего файла увидим лишь такое:

```yaml
app:
  password:
    _default: 10006755d101c5243fc400ababd7358689a921c19ee7e96a95f0ab82d46e4424573ab50ba666fcf5ce5e5dbd2b696c7706cf
    production: 1000bcd51061ebd1b2c2990041d30783be607b3a0aec8890c098f17bc96dc43e93765219651d743c7a37fb7361c10b703c7b
```
И в таком виде уже безопасно хранить пароли в git.
Для того чтобы отредактировать значение нужно снова воспользоваться командой

```bash
werf helm secret values edit .helm/secret-values.yaml
```

Для дальнейшего его использования - для деплоя приложения - в рамках gitlab нужно ключ WERF_SECRET_KEY положить в gitlab variables для проекта (Settings -> CI/CD -> variables). Оттуда werf при запуске получит эту перменную и сформирует корректный helm-chart с расшифрованным паролем.



#### Деплой в Gitlab CI

Опишем деплой приложения в Kubernetes. Деплой будет осуществляться на два стенда: staging и production.

Выкат на два стенда отличается только параметрами, поэтому воспользуемся шаблонами. Опишем базовый деплой, который потом будем кастомизировать под стенды: 

```yaml
.base_deploy: &base_deploy
  script:
    - werf deploy --stages-storage :local
  dependencies:
    - Build
  tags:
    - article-werf
```

Выкат, например, на Staging, будет выглядеть так: 
 
 ```yaml
 Deploy to Stage:
   extends: .base_deploy
   stage: deploy
   environment:
     name: stage
   except:
     - schedules
   only:
     - merge_requests
   when: manual
```

Нет необходимости пробрасывать переменные окружения, создаваемые GitLab CI — этим занимается Werf. Достаточно только указать название стенда

```yaml
environment:
     name: stage
```

_Обратите внимание: домены каждого из стендов указываются в helm-шаблонах._

_Остальные настройки подробно описывать не будем, разобраться в них можно с [помощью документации Gitlab](https://docs.gitlab.com/ce/ci/yaml/)_

После описания стадий выката при создании Merge Request и будет доступна кнопка Deploy to Stage.

![alt_text](images/-6.png "image_tooltip")

Посмотреть статус выполнения pipeline можно в интерфейсе gitlab **CI / CD - Pipelines**

![alt_text](images/-7.png "image_tooltip")


Список всех окружений - доступен в меню **Operations - Environments**

![alt_text](images/-8.png "image_tooltip")

Из этого меню - можно так же быстро открыть приложение в браузере.

{{И тут в итоге должна быть картинка как аппка задеплоилась и объяснение картинки}}

# Подключаем зависимости

Werf подразумевает, что лучшей практикой будет разделить сборочный процесс на этапы, каждый с четкими функциями и своим назначением. Каждый такой этап соответствует промежуточному образу, подобно слоям в Docker. В werf такой этап называется стадией, и конечный образ в итоге состоит из набора собранных стадий. Все стадии хранятся в хранилище стадий, которое можно рассматривать как кэш сборки приложения, хотя по сути это скорее часть контекста сборки.

Стадии — это этапы сборочного процесса, кирпичи, из которых в итоге собирается конечный образ. Стадия собирается из группы сборочных инструкций, указанных в конфигурации. Причем группировка этих инструкций не случайна, имеет определенную логику и учитывает условия и правила сборки. С каждой стадией связан конкретный Docker-образ. Подробнее о том, какие стадии для чего предполагаются можно посмотреть в [документации](https://ru.werf.io/documentation/reference/stages_and_images.html).

Werf предлагает использовать для стадий следующую стратегию:

*   использовать стадию beforeInstall для инсталляции системных пакетов;
*   использовать стадию install для инсталляции системных зависимостей и зависимостей приложения;
*   использовать стадию beforeSetup для настройки системных параметров и установки приложения;
*   использовать стадию setup для настройки приложения.

Подробно про стадии описано в [документации](https://ru.werf.io/documentation/configuration/stapel_image/assembly_instructions.html).

В Java, в частности в spring, в качестве менеджера зависимостей может использоваться maven, gradle. Мы будем, как и ранее использовать maven, но для gradle кроме самих команд, логика сборки не поменяется. Мы уже описывали ранее его использование в файле `werf.yaml` но сейчас оптимизируем его использование.

## Подключение менеджера зависимостей

В maven используется pom.xml в качестве файла проекта где, помимо мета-информации, описываются зависимости. Вот так мы к нему обращались когда собирали приложение ранее:

```yaml
    shell: |
      mvn -B -f pom.xml package dependency:resolve
```

Однако, если оставить всё так — стадия `beforeInstall` не будет запускаться при изменении pom.xml. Подобная зависимость пользовательской стадии от изменений [указывается с помощью параметра git.stageDependencies](https://ru.werf.io/documentation/configuration/stapel_image/assembly_instructions.html#%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D1%8C-%D0%BE%D1%82-%D0%B8%D0%B7%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9-%D0%B2-git-%D1%80%D0%B5%D0%BF%D0%BE%D0%B7%D0%B8%D1%82%D0%BE%D1%80%D0%B8%D0%B8):

```yaml
git:
- add: /
  to: /app
  stageDependencies:
    setup:
    - pom.xml
```

При изменении файла pom.xml стадия `setup` будет запущена заново.
Почитать о том как формируется pom.xml вручную можно [здесь](https://maven.apache.org/guides/getting-started/maven-in-five-minutes.html). В нашем случае, для spring мы снова воспользуемся веб-интерфейсом [https://start.spring.io/](https://start.spring.io/), который использовали для формирования hello-world приложения. Плюсом этого метода безусловано является его быстрота, возможность избежать глупых ошибок при написании pom.xml.

## Оптимизация сборки

В случае с spring даже в пустом проекте сборщику нужно скачать приличное количество файлов, которое происходит не за нулевое время. И скачивать эти файлы раз за разом выглядит нецелесообразным, тем более что maven (и gradle) позволяет этот кеш переиспользовать при локальной сборке.
Для оптимизации скорости сборки у werf есть несколько механизмов.
Рассмотрим сначала возможность переиспльзовать кеш, который скачивает maven из хост-системы в контейнер. В werf есть механизм переиспользования так называемого build_dir между сборками в одном проекте.
Рассмотрим что же нам можно переиспользовать. Maven складывает свой локальный репозиторий в `{userhome}/.m2/repository` (внутри контейнера, разумеется). Его мы и пошарим между сборками используя build_dir.
Для этого в werf служит директива mount:


```yaml
...
git:
- add: /
  to: /app
  stageDependencies:
    setup:
    - pom.xml
mount:
- from: build_dir
  to: /root/.m2/repository
...
```

Как упоминалось выше - werf кеширует успешные стадии деплоя, что существенно ускоряет сборки. А maven позволяет отдельно выполнить команды resolve зависимостей и выполнять сборку приложения. Вынесем resolve зависимостей на отдельную - предыдущую - стадию. В этом случае, при изменении только кода, а не pom.xml werf увидит слой с зависимостями не поменялся и возьмет его из кеша, а затем уже проведет сборку приложения используя измененный код. Вынесем скачивание локального репозитория в стадию beforeSetup. И настроим стадию setup на сборку в случае изменений чего-либо в папке src, где и лежит сам код:

```
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
git:
- add: /
  to: /app
  stageDependencies:
    beforeSetup:
    - pom.xml
    setup:
    - src
mount:
- from: build_dir
  to: /root/.m2/repository
ansible:
  beforeSetup:
  - name: dependency resolve
    shell: |
      mvn -B -f pom.xml dependency:resolve
    args:
      chdir: /app
      executable: /bin/bash
  setup:
  - name: Build jar
    shell: |
      mvn -B -f pom.xml package
    args:
      chdir: /app
      executable: /bin/bash
```

Теперь первая сборка - без кешей - будет сравнительно долгой. Будут скачиваться репозитории maven, описанные в pom.xml, затем из кода в src будет собираться приложение. Напомню о механизмах отладки на этом этапе -

```shell
--introspect-before-error
--introspect-error
```

И в случае проблем мы сможем попасть внутрь контейнера и выполнить команды сборки вручную.

На этом этапе у нас получилось оптимизировать скорость сборки - кеш каждый раз не скачивается, стадия не перезапускается при изменении только кода. Однако базовый образ maven:3-jdk-8, который мы использовали для сборки достаточно тяжелый, нет смысла запускать все это в kubernetes. 
Будем запускать используя достаточно легкий openjdk:8-jdk-alpine. Но нам все еще нужно собирать jar в образе с maven. Для реализации этого решения воспользуемся [артефактом](https://werf.io/documentation/configuration/stapel_artifact.html). По сути это то же самое что и image в директивах werf.yaml, только временный. Он не пушится в registry.
Переименуем image в artifact и назовем его build. Результатом его работы является собранный jar - именно его мы и импортируем в image с alpine-openjdk который опишем в этом же werf.yaml после "---", которые разделяют image. Назовем его spring и уже его пушнем в registry для последующего запуска в кластере.
Для импорта между image и artifact служит директива import. Из /app/target в сборочном артефакте импортируем собранный jar-файл в папку /app в image spring. Единственное что следует еще поправить - это версию собираемого jar в pom.xml. Пропишем её 1.0, чтобы имя итогового jar-файла получось предсказуемым - demo-1.0.jar. Итоговый werf.yaml:

```yaml
project: spring
configVersion: 1
---
artifact: hello
from: maven:3-jdk-8
git:
- add: /
  to: /app
  stageDependencies:
    beforeSetup:
    - pom.xml
    setup:
    - src
mount:
- from: build_dir
  to: /root/.m2/repository
ansible:
  beforeSetup:
  - name: dependency resolve
    shell: |
      mvn -B -f pom.xml dependency:resolve
    args:
      chdir: /app
      executable: /bin/bash
  setup:
  - name: Build jar
    shell: |
      mvn -B -f pom.xml package
    args:
      chdir: /app
      executable: /bin/bash
---
image: hello
from: openjdk:8-jdk-alpine
import:
- artifact: build
  add: /app/target/demo-1.0.jar
  to: /app/app.jar
  after: setup

```

# Генерируем и раздаем ассеты

В какой-то момент в процессе разработки вам понадобятся ассеты (т.е. картинки, css, js).

Для генерации ассетов мы будем использовать webpack.
Генерировать ассеты для java-spring-maven можно, конечно, разными способами. Например, в maven есть [плагин](https://github.com/eirslett/frontend-maven-plugin), который позволяет описать сборку ассетов "не выходя" из Java. Но там есть несколько оговорок про use-case этого плагина:

*   Не предполагается использовать как замена Node для разработчиков фронтенда. Скорее для того чтобы разработчики бекенда могли быстрее включить JS-код в свою сборку.
*   Не предполагается использование на production-окружениях.

Потому хорошим и распространенным выбором будет использовать webpack отдельно.

Интуитивно понятно, что на стадии сборки нам надо будет вызвать скрипт, который генерирует файлы, т.е. что-то надо будет дописать в `werf.yaml`. Однако, не только там — ведь какое-то приложение в production должно непосредственно отдавать статические файлы. Мы не будем отдавать файлики с помощью {{Frameworkname}}. Хочется, чтобы статику раздавал nginx. А значит надо будет внести какие-то изменения и в helm чарты.

## Сценарий сборки ассетов

webpack - гибкий в плане реализации ассетов инструмент. Настраивается его поведение в webpack.config.js и package.json.
Создадим в этом же проекте папку assets. В ней следующая структура

```
├── default.conf
├── dist
│   └── index.html
├── package.json
├── src
│   └── index.js
└── webpack.config.js
```

При настройке ассетов есть один нюанс - при сборке приложения мы не рекомендуем использовать какие-либо изменяемые переменные. Потому что собранный бинарный образ должен быть независимым от конкретного окружения. А значит во время сборки у нас не может быть, например, указано домена для которого производится сборка, user-generated контента и подобных вещей.

В связи с описанным выше, все ассеты должны быть сгенерированы одинаково для всех окружений. А вот использовать их стоит в зависимости от окружения в котором запущено приложение.

## Какие изменения необходимо внести

Генерация ассетов происходит в отдельном артефакте в 2-х стадиях - `install` и `setup`. На первой стадии мы выполняем `npm install`, на второй уже `npm build`.
Так же нам потребуются изменения в шаблонах helm. Нам нужно будет описать процесс запуска контейнера с ассетами. Мы не будем их класть в контейнер с приложением. Запустим отдельный контейнер с nginx и в нем уже будем раздавать статику. Соответственно нам потребуются изменения в deployment. Так же потребуется добавить конфиг nginx. Его мы добавим, описав configmap.
И не забудем про маршрутизацию трафика до ассетов. Потребуются правки в ingress и service.
Обо всем этом ниже.

### Изменения в сборке

В конфиги сборке (werf.yaml) потребуется описать артефакт для сборки и импорт результатов сборки в контейнер с nginx:

ВСТАВИТЬ ССЫЛКИ НА КОД

Все аналогично сборкам, которые мы проводили ранее.
Для артефакта настроили пересборку `npm install` в случае изменения `package.json` и `webpack.config.js`. А так же запуск `npm run build` при любом изменении файла в репозитории.

### Изменения в деплое

Запустим наш контейнер с nginx в том же поде с приложением но в отдельном контейнере. 



В deployment допишем:

```yaml
      - name: frontend
{{ tuple "frontend" . | include "werf_container_image" | indent 8 }}
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/conf.d/default.conf
          subPath: default.conf
        - name: env-js
          mountPath: /app/dist/env.js
          subPath: env.js
...
...
      volumes:
      - name: nginx-config
        configMap:
          name: {{ .Chart.Name }}-configmap
      - name: env-js
        configMap:
          name: {{ .Chart.Name }}-configmap
```

Здесь мы добавили подключение configmap к deployment. configmap:

```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Chart.Name }}-configmap
data:
  default.conf: |
    server {
        listen       80;
        server_name  localhost;
        location / {
            root   /app/dist/;
            index  index.html index.htm;
        }
    }

  env.js: |
    module.exports = {
        hosturl: {{ pluck .Values.global.env .Values.app.url | first | default .Values.app.url._default |quote }},
    };

```

Чтобы попадать в контейнер с ассетами нужно дописать port 80 в service, ниже (в ingress) мы направим трафик предназначенный для ассетов в этот контейнер.

```yaml
...
  - name: http-front
    port: 80
    targetPort: 80
```


### Изменения в роутинге

В ingress направим все что связано с ассетами на порт 80, чтобы все запросы к //mywebsite/static/ попали в нужный контейнер на 80ый порт, где уже у нас будет отвечать nginx, прописанный выше.

```yaml
...
      - path: /static/
        backend:
          serviceName: {{ .Chart.Name | quote }}
          servicePort: 80
```

# Работа с файлами

Файлы – варианта два:



1. У вас в кубе есть сетефая файловая система (EFS, NFS, …), которая позволяет подключить общую директорию ко многим подам одновременно. Тогда можно работать по-старинке.
2. Правильный вариант – S3.

# Работа с электронной почтой

{{Почта – один вариант, используем внешнее API.}}

# Подключаем redis

Допустим к нашему приложению нужно подключить простейшую базу данных, например, redis или memcached. Возьмем первый вариант.

В простейшем случае нет необходимости вносить изменения в сборку — всё уже собрано для нас. Надо просто подключить нужный образ, а потом в вашем Java-приложении корректно обратиться к этому приложению.

## Завести Redis в Kubernetes

Есть два способа подключить: прописать helm-чарт самостоятельно или подключить внешний. Мы рассмотрим второй вариант.

Подключим redis как внешний subchart.

Для этого нужно:

1. прописать изменения в yaml файлы;
2. указать редису конфиги
3. подсказать werf, что ему нужно подтягивать subchart.

Добавим в файл `.helm/requirements.yaml` следующие изменения:

```yaml
dependencies:
- name: redis
  version: 9.3.2
  repository: https://kubernetes-charts.storage.googleapis.com/
  condition: redis.enabled
```

Для того чтобы werf при деплое загрузил необходимые нам сабчарты - нужно добавить команды в `.gitlab-ci`

```yaml
.base_deploy:
  stage: deploy
  script:
    - werf helm repo init
    - werf helm dependency update
    - werf deploy
```

Опишем параметры для redis в файле `.helm/values.yaml`

```yaml
redis:
  enabled: true
```

При использовании сабчарта по умолчанию создается master-slave кластер redis.

Если посмотреть на рендер (`werf helm render`) нашего приложения с включенным сабчартом для redis, то можем увидеть какие будут созданы сервисы:

```yaml
# Source: example-2/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-master

# Source: example-2/charts/redis/templates/redis-slave-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-slave
```

## Подключение Java приложения к базе redis

В нашем приложении - мы будем  подключаться к мастер узлу редиса. Нам нужно, чтобы при выкате в любое окружение приложение подключалось к правильному редису.

Рассмотрим настройки подключения к redis из нашего приложения.

Тут дальше описываем как к какому порту подрубиться, чтобы подключиться. Возможно даже — куда мы и что пишем в приложении, чтоб

# Подключаем базу данных

Тут будут единые примеры, которые во всех статьях используются. Ибо для сборки и деплоя как-то не сильно разно получается, и хельм одинаковый.


## Как подключить БД \
 \
Рассмотрим тот же spring-boot, но уже с mysql. За основу возьмем [https://spring.io/guides/gs/accessing-data-mysql/](https://spring.io/guides/gs/accessing-data-mysql/). Возьмем за основу тот же werf.yaml, что использовался выше, только поменяем имя генерируемого jar для соответствия коду с сайта. 

Единственное отличие с инструкцией spring будет в файле `src/main/resources/application.properties - `будем брать хост, пароль, бд и имя пользователя из переменных окружения:` \
`


```
spring.datasource.url=jdbc:mysql://${MYSQL_HOST:localhost}:3306/${MYSQL_DATABASE}
spring.datasource.username=${MYSQL_USER}
spring.datasource.password=${MYSQL_PASSWORD}
```


` \
`Пропишем их в values.yaml: \
 \



```
infr:
 mysql:
   host:
     _default: mysql
   db:
     _default: db_example
   user:
     _default: springuser
   password:
     _default: ThePassword
   rootpassword:
     _default: root
```


` \
`Также нам понадобится сам mysql. Опишем его рядом - в helm-чарте: \
 \



```
apiVersion: apps/v1
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
 name: mysql
spec:
 selector:
   matchLabels:
     service: mysql
 replicas: 1
 serviceName: mysql
 template:
   metadata:
     labels:
       service: mysql
   spec:
     containers:
     - name: mysql
       image: mysql:8.0.16
       args:
       - --default-authentication-plugin=mysql_native_password
       env:
       - name: MYSQL_DATABASE
         value: {{ pluck .Values.global.env .Values.infr.mysql.db | first | default .Values.infr.mysql.db._default }}
       - name: MYSQL_PASSWORD
         value: {{ pluck .Values.global.env .Values.infr.mysql.password | first | default .Values.infr.mysql.password._default }}
       - name: MYSQL_ROOT_PASSWORD
         value: root
       - name: MYSQL_USER
         value: {{ pluck .Values.global.env .Values.infr.mysql.user | first | default .Values.infr.mysql.user._default }}
       ports:
       - containerPort: 3306
       volumeMounts:
       - mountPath: /var/lib/mysql
         name: data
     volumes:
     - name: data
       emptyDir: {}
     restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
 name: mysql
 labels:
   service: mysql
spec:
 ports:
 - name: "3306"
   port: 3306
 selector:
   service: mysql
```


Запуск приложения зависит от наличия mysql, добавим init-container для приложения, чтобы приложение не запускалось до появления коннекта к mysql. 

добавится следующая секция в 10-app.yml: \



```
    spec:
     initContainers:
     - name: wait-mysql
       image: alpine:3.9
       command:
       - /bin/sh
       - -c
       - while ! getent ahostsv4 $MYSQL_HOST; do echo waiting for mysql; sleep 2; done
```


Собираем приложение аналогично первому. Проверяем: \
`curl 172.17.0.7:8080/demo/all`


```
[]
curl 172.17.0.7:8080/demo/add -d name=First -d email=someemail@someemailprovider.com
Saved
curl 172.17.0.7:8080/demo/add -d name=Second -d email=some2email@someemailprovider.com     
Saved
curl 172.17.0.7:8080/demo/all |jq
[
 {
   "id": 1,
   "name": "First",
   "email": "someemail@someemailprovider.com"
 },
 {
   "id": 2,
   "name": "Second",
   "email": "some2email@someemailprovider.com"
 }
]
```



## Выполнение миграций

TODO:  \
 \
[https://www.liquibase.org/get_started/quickstart_sql.html](https://www.liquibase.org/get_started/quickstart_sql.html)


## Накатка фикстур при первом выкате

При использовании spring фреймворк сам следит за наличием нужного в БД и при пустой БД создает все что нужно сам. При наличии прав, разумеется. Разработчики рекомендуют отбирать подобные права у пользователя spring после изначальной инициализации приложения.


# Юнит-тесты и Линтеры

[https://spring.io/guides/gs/testing-web/](https://spring.io/guides/gs/testing-web/) unit-test \
[https://spring.io/guides/gs/maven/](https://spring.io/guides/gs/maven/) unit-test

[https://maven.apache.org/plugins/maven-checkstyle-plugin/usage.html](https://maven.apache.org/plugins/maven-checkstyle-plugin/usage.html) lint

Юниты выполняются при сборк, другие типы тестов могут и должны выполняться на других стадиях. Пока это в доке отстутствует, будет в будущем.


# Несколько приложений в одной репе



1. Добавляем кронджоб
2. Добавляем воркер/консюмер
3. Добавляем вторую приложуху на другом языке (например, это может быть webscoket’ы на nodejs; показать организацию helm, организацию werf.yaml, и ссылку на другую статью). Генерация ассетов подойдет?


# Динамические окружения

TODO: 
