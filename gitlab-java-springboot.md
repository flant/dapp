---
author_team: "charlie"
author_name: "Евгений Ермонин"
ci: "gitlab"
language: "java"
framework: "springboot"
is_compiled: 1
package_managers_possible:
 - maven
 - gradle
package_managers_chosen: "junit"
unit_tests_possible:
 - junit
 - testng
unit_tests_chosen: "flask-sqlalchemy"
assets_generator_possible:
 - webpack
assets_generator_chosen: "webpack"
---

# Чек-лист готовности статьи
<ol>
<li>Все примеры кладём в <a href="https://github.com/flant/examples">https://github.com/flant/examples</a>

<li>Для каждой статьи может и должно быть НЕСКОЛЬКО примеров, условно говоря — по примеру на главу это нормально.

<li>Делаем примеры И на Dockerfile, И на Stapel

<li>Про хельм говорим, про особенности говорим, но в подробности не вдаёмся — считаем, что человек умеет в кубовые ямлы.

<li>Обязательно тестируйте свои примеры перед публикацией
</li>
</ol>

# Введение

Рассмотрим разные способы которые помогут собрать Java-приложение на примере springboot и запустить его в kubernetes кластере.

Предполагается что читатель имеет базовые знания в разработке на Java а также немного знаком с Gitlab CI и примитивами kubernetes, либо готов во всём этом разобраться самостоятельно. Мы постараемся предоставить все ссылки на необходимые ресурсы, если потребуется приобрести какие то новые знания.  

Собирать приложения будем с помощью werf. Данный инструмент работает в Linux MacOS и Windows, инструкция по [установке](https://ru.werf.io/documentation/guides/installation.html) находится на официальном [сайте](https://ru.werf.io/). В качестве примера - также приложим Docker файлы.

Для иллюстрации действий в данной статье - создан репозиторий с исходным кодом, в котором находятся несколько простых приложений. Мы постараемся подготовить примеры чтобы они запускались на вашем стенде и постараемся подсказать, как отлаживать возможные проблемы при вашей самостоятельной работе.


## Подготовка приложения

Наилучшим образом приложения будут работать в Kubernetes - если они соответствуют [12 факторам heroku](https://12factor.net/). Благодаря этому - у нас в kubernetes работают stateless приложения, которые не зависят от среды. Это важно, так как кластер может самостоятельно переносить приложения с одного узла на другой, заниматься масштабированием и т.п. — и мы не указываем, где конкретно запускать приложение, а лишь формируем правила, на основании которого кластер принимает свои собственные решения.

Договоримся что наши приложения соответствуют этим требованиям. На хабре уже было описание данного подхода, вы можете почитать про него например [тут](https://12factor.net/).


## Подготовка и настройка среды

Для того, чтобы пройти по этому гайду, необходимо, чтобы

*   У вас был работающий и настроенный Kubernetes кластер
*   Код приложения находился в Gitlab
*   Был настроен Gitlab CI, подняты и подключены к нему раннеры

Для пользователя под которым будет производиться запуск runner-а - нужно установить multiwerf - данная утилита позволяет переключаться между версиями werf и автоматически обновлять его. Инструкция по установке - доступна по [ссылке](https://ru.werf.io/documentation/guides/installation.html#installing-multiwerf).

Для автоматического выбора актуальной версии werf в канале stable, релиз 1.1 выполним следующую  команду:

```
. $(multiwerf use 1.1 stable --as-file)
```

Перед деплоем нашего приложения необходимо убедиться что у нас подготовлены инфраструктурные компоненты:

*   К gitlab подключен shell runner с тегом werf. [Инструкция](https://ru.werf.io/documentation/guides/gitlab_ci_cd_integration.html#%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-runner) по подготовке gitlab runner.
*   Ранеры включены и активны для репозитория с нашим приложением
*   Для пользователя под которым запускается сборка и деплой установлен kubectl и добавлен конфигурационный файл для подключения к kubernetes.
*   Для gitlab включен и настроен gitlab registry
*   Раннер запущен на отдельной виртуалке, имеет доступ к API kubernetes и запускается по тегу werf

## Настройка Gitlab Runner

{{TODO: откорректировать то, что ниже}}

Теперь обязательно на сервере с gitlab-runner, который занимается сборкой вашего приложения, установить [kubectl](https://kubernetes.io/ru/docs/tasks/tools/install-kubectl/). И положить в домашнюю директорию пользователя gitlab-runner конфиг kubernetes в папку `.kube` (нужно её создать, если её нет)

Конфиг можно взять на мастере кластера kubernetes в `/etc/kubernetes/admin.conf`

Скопируйте его и положите в папку `.kube` переименовав файл в `config`.

![alt_text](images/-5.png "image_tooltip")

Обычно в конфиге указан прямой адрес мастера kubernetes. Соответственно нужно чтобы мастер нашего кластера был доступен по сети для gitlab-runner.

Всё это мы сделали для того чтобы werf мог общаться с API kubernetes и деплоить в него наши приложения, это и есть второй ответ на вопрос “Как werf понимает куда ему нужно деплоить?”. По умолчанию деплой будет происходить в namespace состоящий из имени проекта задаваемого в `werf.yaml` и окружения задаваемого в `.gitlab-ci.yml` куда мы деплоим наше приложение.
# Hello world

В первой главе мы покажем поэтапную сборку и деплой приложения без задействования внешних ресурсов таких как база данных и сборку ассетов.

Наше приложение будет состоять из одного docker образа собранного с помощью werf. Его единственной задачей будет вывод сообщения “hello world” по http.

В нашем случае будет работать процесс java, исполняющий собранный jar отдающий hello world по http.

Управлять маршрутизацией запросов к приложению будет управлять Ingress в kubernetes кластере.

Мы реализуем два стенда: production и staging. В рамках hello world приложения мы предполагаем, что разработка ведётся локально, на вашем компьютере.

_В ближайшее время werf реализует удобные инструменты для локальной разработки, следите за обновлениями._


## Локальная сборка

Для примера сборки возьмем инструкцию [https://spring.io/guides/gs/spring-boot-docker/](https://spring.io/guides/gs/spring-boot-docker/) и сам исходный код вот отсюда [https://github.com/spring-guides/gs-spring-boot-docker](https://github.com/spring-guides/gs-spring-boot-docker)

Для того чтобы werf смогла начать работу с нашим приложением - необходимо в корне нашего репозитория создать файл werf.yaml в которым будут описаны инструкции по сборке. Для начала соберем образ локально не загружая его в registry чтобы разобраться с синтаксисом сборки.

С помощью werf можно собирать образы с используя Dockerfile или используя синтаксис, описанный в документации werf (мы называем этот синтаксис и движок, который этот синтаксис обрабатывает, stapel). Для лучшего погружения - соберем наш образ с помощью stapel.

Итак, начнём с самой главной секции нашего werf.yaml файла, которая должна присутствовать в нём **всегда**. Называется она [meta config section](https://werf.io/documentation/configuration/introduction.html#meta-config-section) и содержит всего два параметра.

werf.yaml:
```yaml
project: spring
configVersion: 1
```

**_project_** - поле, задающее имя для проекта, которым мы определяем связь всех docker images собираемых в данном проекте. Данное имя по умолчанию используется в имени helm релиза и имени namespace в которое будет выкатываться наше приложение. Данное имя не рекомендуется изменять (или подходить к таким изменениям с должным уровнем ответственности) так как после изменений уже имеющиеся ресурсы, которые выкачаны в кластер, не будут переименованы.

**_configVersion_** - в данном случае определяет версию синтаксиса используемую в `werf.yaml`.

После мы сразу переходим к следующей секции конфигурации, которая и будет для нас основной секцией для сборки - [image config section](https://werf.io/documentation/configuration/introduction.html#image-config-section). И чтобы werf понял что мы к ней перешли разделяем секции с помощью тройной черты.


```yaml
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
```

image задает короткое имя собираемого docker-образа. Должно быть уникально в рамках одного werf-файла.  \
from - аналогичная секция с обычным dockerfile.

В примере spring используется `openjdk:8-jdk-alpine, `но он хорош для запуска, мы же воспользуемся образом в котором уже предустановлены все что необходимо maven для сборки - `maven:3-jdk-8.`

Теперь встает вопрос о том как нам добавить исходный код приложения внутрь нашего docker image. И для этого мы можем использовать Git! И нам даже не придётся устанавливать его внутрь docker image.

**_git_**, на наш взгляд это самый правильный способ добавления ваших исходников внутрь docker image, хотя существуют и другие. Его преимущество в том что он именно клонирует, и в дальнейшем накатывает коммитами изменения в тот исходный код что мы добавили внутрь нашего docker image, а не просто копирует файлы. Вскоре мы узнаем зачем это нужно.


```yaml
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
git:
- add: /
  to: /app
```

Werf подразумевает что ваша сборка будет происходить внутри директории склонированного git репозитория. Потому мы списком можем указывать директории и файлы относительно корня репозитория которые нам нужно добавить внутрь image.

`add: /` - та директория которую мы хотим добавить внутрь docker image, мы указываем, что это весь наш репозиторий

`to: /app` - то куда мы клонируем наш репозиторий внутри docker image. Важно заметить что директорию назначения werf создаст сам.

 Есть возможность даже добавлять внешние репозитории внутрь проекта не прибегая к предварительному клонированию, как это сделать можно узнать [тут](https://werf.io/documentation/configuration/stapel_image/git_directive.html), но мы не рекомендуем такой подход.

Далее описываются стадии сборки а также какой именно сборщик используется. Начнем со второго - сейчас доступно 2 вида сборщика - shell и ansible. Первый аналогичен директиве RUN в dockerfile. Его удобнее использовать для быстрого получения результата с минимальными затратами времени на изучение. ansible более молодой инструмент и требующий несколько большего времени на изучение, но он позволяет получить более прогнозируемый результат вследствии декларативности. \
 Теперь приведу выдержку из [документации](https://werf.io/documentation/configuration/stapel_image/assembly_instructions.html#usage-of-user-stages) - werf позволяет определять до четырех _пользовательских стадий_ с инструкциями сборки. На содержание самих инструкций сборки werf не накладывает каких-либо ограничений, т.е. вы можете указывать все те же инструкции, которые указывали в Dockerfile в директиве `RUN`. Однако важно не просто перенести инструкции из Dockerfile, а правильно разбить их на _пользовательские стадии_. Мы предлагаем такое разбиение исходя из опыта работы с реальными приложениями, и вся суть тут в том, что большинство сборок приложений проходят следующие этапы:



1. установка системных пакетов
2. установка системных зависимостей
3. установка зависимостей приложения
4. настройка системных пакетов \
настройка приложения

 Шаблон _пользовательских стадий_ предлагает следующую стратегию:



1. использовать стадию _beforeInstall_ для инсталляции системных пакетов;
2. использовать стадию _install_ для инсталляции системных зависимостей и зависимостей приложения;
3. использовать стадию _beforeSetup_ для настройки системных параметров и установки приложения; \
использовать стадию _setup_ для настройки приложения.

Несмотря на изложенную четкую стратегию шаблона _пользовательских стадий_ и назначения каждой стадии, по сути нет никаких ограничений. Предложенные назначения каждой стадии являются лишь рекомендацией, которые основаны на нашем анализе работы реальных приложений. Вы можете использовать только одну пользовательскую стадию, либо определить свою стратегию группировки инструкций, чтобы получить преимущества кэширования и зависимостей от изменений в git-репозиториях с учетом особенностей сборки вашего приложения.

Опишем пример из spring boot docker example в werf. Для запуска приложения предлагают использовать следующий dockerfile: \



```
FROM openjdk:8-jdk-alpine
ARG JAR_FILE=target/*.jar
COPY ${JAR_FILE} app.jar
ENTRYPOINT ["java","-jar","/app.jar"]
```


Однако, чтобы запускать jar его нужно предварительно собрать. Предлагается сделать это локально, мы же соберем jar так же используя werf и ansible-сборшик. Поскольку все системные зависимости для сборки удовлетворены - мы используем образ openjdk с maven - опишем сборку приложения в стадии setup:


```
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
git:
- add: /
  to: /app
ansible:
 setup:
 - name: Build jar
   shell: |
     mvn -B -f pom.xml package dependency:resolve
   args:
     chdir: /app
     executable: /bin/bash
```


Уже сейчас можем запустить сборку и получить docker-образ с лежащим внутри jar.  \


Полный список поддерживаемых модулей ansible в werf можно найти [тут](https://werf.io/documentation/configuration/stapel_image/assembly_instructions.html#supported-modules).

Не забыв [установить werf](https://werf.io/documentation/guides/installation.html) локально, запускаем сборку с помощью [werf build](https://werf.io/documentation/cli/main/build.html)!

```
$  werf build --stages-storage :local
```


TODO: картинка? \

![alt_text](images/-werf0.gif "image_tooltip")

Вот и всё, наша сборка успешно завершилась. К слову если сборка падает и вы хотите изнутри контейнера её подебажить вручную, то вы можете добавить в команду сборки флаги:

```
--introspect-before-error
```

или

```
--introspect-error
```

Которые при падении сборки на одном из шагов автоматически откроют вам shell в контейнер, перед исполнением проблемной инструкции или после.

В конце werf отдал информацию о готовом image:

![alt_text](images/-1.png "image_tooltip")

Теперь его можно запустить локально используя image_id просто с помощью docker.
Либо вместо этого использовать [werf run](https://werf.io/documentation/cli/main/run.html):


```
werf run --stages-storage :local --docker-options="-d -p 8080:8080 --restart=always" -- java -jar /app/app.jar
```

Первая часть команды очень похожа на build, а во второй мы задаем [параметры](https://docs.docker.com/engine/reference/run/) docker и через двойную черту команду с которой хотим запустить наш image.

Небольшое пояснение про `--stages-storage :local `который мы использовали и при сборке и при запуске приложения. Данный параметр указывает на то где werf хранить стадии сборки. На момент написания статьи это возможно только локально, но в ближайшее время появится возможность сохранять их в registry.

Теперь наше приложение доступно локально на порту 8080:

![alt_text](images/-2.png "image_tooltip")

На этом часть с локальным использованием werf мы завершаем и переходим к той части для которой werf создавался, использовании его в CI.

## Построение CI-процесса

После того как мы закончили со сборкой, которую можно производить локально, мы приступаем к базовой настройке CI/CD на базе Gitlab.

Начнем с того что добавим нашу сборку в CI с помощью .gitlab-ci.yml, который находится внутри корня проекта. Нюансы настройки CI в Gitlab можно найти [тут](https://docs.gitlab.com/ee/ci/).

Мы предлагаем простой флоу, который мы называем [fast and furious](https://docs.google.com/document/d/1a8VgQXQ6v7Ht6EJYwV2l4ozyMhy9TaytaQuA9Pt2AbI/edit#). Такой флоу позволит вам осуществлять быструю доставку ваших изменений в production согласно методологии GitOps и будут содержать два окружения, production и stage.

На стадии сборки мы будем собирать образ с помощью werf и загружать образ в registry, а затем на стадии деплоя собрать инструкции для kubernetes, чтобы он скачивал нужные образы и запускал их.

### Сборка в Gitlab CI

Для того, чтобы настроить CI-процесс создадим .gitlab-ci.yaml в корне репозитория

{{Версию верфи выносим в переменную}}

{{Сборку стадий прописываем в WERF_STAGES_STORAGE}}

{{Выносим подключение верфи в before_script (вот эти type multiwerf и type werf)}}

{{В самой билд стадии делаем werf build-and-publish}}

{{Если есть компиляция — описываем как её делать}}

{{Про content-based tagging}}

{{Отдельно проговариваем историю с проброской конфигов в стадию сборки.}} 

Теперь мы можем запушить наши изменения и увидеть что наша стадия успешно выполнилась.

![alt_text](images/-3.png "image_tooltip")


Лог в Gitlab будет выглядеть так же как и при локальной сборке, за исключением того что в конце мы увидим как werf пушит наш docker image в registry.

```
207 │ ┌ Publishing image {{node}} by stages-signature tag c905b748cb9647a03476893941837bf79910ab09e ...
208 │ ├ Info
209 │ │   images-repo: registry.gitlab-example.com/{{chat/node}}
210 │ │        image: registry.gitlab-example.com/{{chat/node}}:c905b748cb9647a03476893941 ↵
211 │ │   837bf79910ab09ef5878037592a45d
212 │ └ Publishing image {{node}} by stages-signature tag c905b748cb9647a0347689394 ... (14.90 seconds)
213 └ ⛵ image {{node}} (73.44 seconds)
214 Running time 73.47 seconds
218 Job succeeded
```

### Деплой

werf использует встроенный Helm для применения конфигурации в Kubernetes. Для описания объектов Kubernetes werf использует конфигурационные файлы Helm: шаблоны и файлы с параметрами (например, values.yaml). Помимо этого, werf поддерживает дополнительные файлы, такие как файлы c секретами и с секретными значениями (например secret-values.yaml), а также дополнительные Go-шаблоны для интеграции собранных образов.

Werf (по аналогии с helm) берет yaml шаблоны, генерирует из них  огромную простыню с финальными ямлами, куда подставлены все значения. В этой простыне ямла — аннотации для кубернетеса. Эта простыня закидывается в кубернетес кластер, который парсит инструкции в ямле и вносит изменения в кластер. Верфь смотрит за тем, как кубернетес вносит изменения и дожидается, чтобы реально всё было применено.

Внутри Werf доступны команды Helm-а, например, проверить какие файлы получаются в результате работы werf с шаблонами можно выполнив команду рендер:

```
$ werf helm render
```

Аналогично, доступны команды [helm list](https://werf.io/documentation/cli/management/helm/list.html) и другие.

#### Общее про хельм-конфиги

На сегодняшний день [Helm](https://helm.sh/) один из самых удобных способов которым вы можете описать свой deploy в Kubernetes. Кроме возможности установки готовых чартов с приложениями прямиком из репозитория, где вы можете введя одну команду, развернуть себе готовый Redis, Postgres, Rabbitmq прямиком в Kubernetes, вы также можете использовать Helm для разработки собственных чартов с удобным синтаксисом для шаблонизации выката ваших приложений.

Потому для werf это был очевидный выбор использовать такую технологию.

Мы не будем вдаваться в подробности разработки yaml манифестов с помощью Helm для Kubernetes. Осветим лишь отдельные её части, которые касаются данного приложения и werf в целом. Если у вас есть вопросы о том как именно описываются объекты Kubernetes, советуем посетить страницы документации по Kubernetes с его [концептами](https://kubernetes.io/ru/docs/concepts/) и страницы документации по разработке [шаблонов](https://helm.sh/docs/chart_template_guide/) в Helm.

Нам понадобятся следующие файлы со структурой каталогов:


```
.helm (здесь мы будем описывать деплой)
├── templates (объекты kubernetes в виде шаблонов)
│   ├── deployment.yaml (основное приложение)
│   ├── ingress.yaml (описание для ingress)
│   └── service.yaml (сервис для приложения)
├── secret-values.yaml (файл с секретными переменными)
└── values.yaml (файл с переменными для параметризации шаблонов)
```

Подробнее читайте в [нашей статье](https://habr.com/ru/company/flant/blog/423239/) из серии про Helm.

![alt_text](images/-4.png "image_tooltip")

#### Описание приложения в хельме

Для работы нашего приложения в среде Kubernetes понадобится описать сущности Deployment, Service и завернуть трафик на приложение, донастроив роутинг в кластере.

Не забываем создать валидный ключ для доступа из kubernetes к registry gitlab.

```yaml
      imagePullSecrets:
      - name: registrysecret
```

```
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: {{ .Chart.Name | quote }}
spec:
 replicas: {{ pluck .Values.global.env .Values.app.replicas | first | default .Values.app.replicas._default }}
 selector:
   matchLabels:
     app: {{ .Chart.Name | quote }}
 template:
   metadata:
     labels:
       app: {{ .Chart.Name | quote }}
   spec:
     containers:
     - name: {{ .Chart.Name | quote }}
{{ tuple "hello" . | include "werf_container_image" | indent 8 }}
       command:
       - java
       - -jar
       - /app/app.jar
       - --debug
       workingDir: /app
       ports:
       - name: http
         containerPort: 8080
         protocol: TCP
       env:
{{ tuple "hello" . | include "werf_container_env" | indent 8 }}
---
apiVersion: v1
kind: Service
metadata:
 name: {{ .Chart.Name | quote }}
spec:
 selector:
   service: {{ .Chart.Name | quote }}
 ports:
 - name: http
   port: 8080
   targetPort: 8080
```


Переменную для количества реплик будем брать из values.yaml, для этого добавим туда следующие строки: \



```
---
app:
 replicas:
   _default: 1
   stage: 2
   production: 2
```


Таким же способом следует определять все переменные которые будут влиять на работу приложения. Например адреса подключения к БД, user, password. Но об этом немного ниже. \
Для дебага приложения также очень важным является его способность писать свои логи в stdout. В нашем случае это работает из коробки, разве что можно добавить разговорчивости приложению, добавив в команду запуска --debug, как показано выше.

_Рассказываем что мы вообще по логике закатывать в хельм._

_Про проброску конфигов через переменные окружения и почему это так._

_Верфёвые команды в хельмах. как оно что оно_

_ОБЯЗАТЕЛЬНО написать про то, что логи в stdout и как это сделать в этом фреймворке (не исключает использования Sentry-подобных штук, о чём будет позже)_


#### Роутинг

Для того чтобы обращаться к нашему приложению извне опишем helm-чарт для ingress-контроллера: \



```
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: {{ .Chart.Name | quote }}
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: {{ .Values.global.ci_url }}
    http:
      paths:
      - path: /
        backend:
          serviceName: {{ .Chart.Name | quote }}
          servicePort: 8080
```


ci_url - переменная, которую мы передаем на этапе деплоя. Передается команде werf deploy через опцию --set ci_url=”hello-world.spring”. Чуть подробнее про это ниже, а сейчас вкратце о том как трафик попадет в нужный нам под. После деплоя приложения в кластер создаются ingress-ресурс, service и под. При запросе hello-world.spring попадаем в нужный нам ingress, в нужный location - /. Он ведет на serviceName по названию чарта (project в werf-файле) на порт 8080. Сервис же ищет поды по описанным в нем лейблам и направляет трафик в нужные поды. Стоит отметить, что если поды не в состоянии running - не успели запуститься/уже упали/проблема с запуском - трафик на эти поды не роутится, что ведет нас к написанию корректных readiness/liveness проб для приложения. Но это выходит за рамки этой статьи. \
_ОБЯЗАТЕЛЬНО отобразить историю с роутингом на приложение (ингрессы, вот это всё) и кратко про ебель с отладкой этого говна_

_Проброска ci_url и какова логика_


#### Секретные переменные

Помимо возможности передать переменную в открытом виде, werf [умеет](https://werf.io/documentation/reference/deploy_process/working_with_secrets.html) шифровать/расшифровывать значения, чтобы можно было без опасений класть пароли в git. Для этого нужно задать переменную WERF_SECRET_KEY или в корень проекта положить .werf_secret_key, разумеется предварительно добавив этот файл в .gitignore.


#### Деплой в Gitlab CI

Опишем деплой приложения в Kubernetes. Деплой будет осуществляться на два стенда: staging и production.

Выкат на два стенда отличается только параметрами, поэтому воспользуемся шаблонами. Опишем базовый деплой, который потом будем кастомизировать под стенды: 

```
.base_deploy: &base_deploy
  script:
    - werf deploy --stages-storage :local
  dependencies:
    - Build
  tags:
    - article-werf
```

Выкат, например, на Staging, будет выглядеть так: 
 
 ```
 Deploy to Stage:
   extends: .base_deploy
   stage: deploy
   environment:
     name: stage
   except:
     - schedules
   only:
     - merge_requests
   when: manual
```

Нет необходимости пробрасывать переменные окружения, создаваемые GitLab CI — этим занимается Werf. Достаточно только указать название стенда

```yaml
environment:
     name: stage
```

_Обратите внимание: домены каждого из стендов указываются в helm-шаблонах._

_Остальные настройки подробно описывать не будем, разобраться в них можно с [помощью документации Gitlab](https://docs.gitlab.com/ce/ci/yaml/)_

После описания стадий выката при создании Merge Request и будет доступна кнопка Deploy to Stage.

![alt_text](images/-6.png "image_tooltip")

Посмотреть статус выполнения pipeline можно в интерфейсе gitlab **CI / CD - Pipelines**

![alt_text](images/-7.png "image_tooltip")


Список всех окружений - доступен в меню **Operations - Environments**

![alt_text](images/-8.png "image_tooltip")

Из этого меню - можно так же быстро открыть приложение в браузере.

{{И тут в итоге должна быть картинка как аппка задеплоилась и объяснение картинки}}

# Подключаем зависимости

Отлично! Мы умеем теперь собирать простое приложение и деплоить его в kubernetes-кластер. Однако вывести hello world - это одно, а вот запустить что-то более крупное выглядит более сложной задачей. Это так, но с точки зрения сборки в werf - лишь отчасти.  \
Для того чтобы начать что-то использовать в java мы должны рассказать об этом нашему сборщику - в нашем случае maven. Для этого служит pom.xml и мы уже использовали ранее когда собирали приложение: \
 


```
   shell: |
     mvn -B -f pom.xml package dependency:resolve
```


 \
В pom.xml лежит вся информация описывающая java-проект. В том числе и зависимости. Вкратце о том как он генерируется штатно можно почитать [здесь](https://maven.apache.org/guides/getting-started/maven-in-five-minutes.html), или же можно для spring воспользоваться веб-интерфейсом [https://start.spring.io/](https://start.spring.io/). Это генератор “скелета” проекта - его оттуда можно будет скачать целиком в виде архива или же утащить только нужные файлы. Если скачать пустой проект, то получим фактически то что имеем сейчас, только добавятся тесты. Но уже в таком виде сборщику приходится скачивать приличное количество файлов, которое происходит далеко не за нулевое время. Для оптимизации скорости сборки у werf есть несколько механизмов. Кеширование успешных стадий сборки, проброс пути с хост системы внутрь контейнера.Рассмотрим сначала возможность пробросить путь который скачивает maven в свой локальный репозиторий из хост-системы в контейнер. А самое главное - переиспользовать эти файлы между сборками. Вообще, werf позволяет монтировать tmp_dir - временную папку, живущую 1 сборку и build_dir - постоянную папку, которую можно переиспользовать в сборках в рамках одного проекта. Сейчас нас интересует именно build_dir, так как интересна именно возможность использовать уже скачанные файлы при последущих сборках проекта. Рассмотрим что же нам можно переиспользовать. Maven складывает свой локальный репозиторий в `{userhome}/.m2/repository` (внутри контейнера, разумеется). Его мы и пошарим между сборками используя build_dir. \
 Для этого в werf служит директива mount:


```
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
git:
- add: /
  to: /app
mount:
- from: build_dir
  to: /root/.m2/repository
ansible:
 setup:
 - name: Build jar
   shell: |
     mvn -B -f pom.xml package dependency:resolve
   args:
     chdir: /app
     executable: /bin/bash
```


 \
Как упоминалось выше - werf кеширует успешные стадии деплоя, что существенно ускоряет сборки. А maven позволяет отдельно выполнить resolve зависимостей и отдельно выполнить сборку приложения. Вынесем resolve зависимостей на отдельную стадию сборки, чтобы при изменении кода, а не pom.xml сборка брала из своих кешей слой с зависимостями, а сборку нового кода проводила отдельно. Вынесем скачивание локального репозитория в стадию beforeSetup. \



```
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
git:
- add: /
  to: /app
mount:
- from: build_dir
  to: /root/.m2/repository
ansible:
  beforeSetup:
  - name: dependency resolve
    shell: |
      mvn -B -f pom.xml dependency:resolve
   args:
     chdir: /app
     executable: /bin/bash
  setup:
  - name: Build jar
    shell: |
      mvn -B -f pom.xml package
    args:
      chdir: /app
      executable: /bin/bash
```


 \
Сейчас наша сборка пройдет отдельную стадию dependency resolve, а затем на стадии setup выполнит сборку jar, используя файлы из предыдущей стадии. Тут есть нюанс - если оставить сборку в таком виде, то пересобираться стадии будут только при изменении werf.yaml, так как werf не знает что ей нужно что-то сделать на определенной стадии сборки. Для этого нужно явно указать [зависимости](https://werf.io/documentation/reference/stages_and_images.html#stage-dependencies) - при изменении каких файлов в репозитории будет проводиться пересборка той или иной стадии. Поставим зависимость стадии beforeSetup от изменений pom.xml, а сборки приложения (стадия setup) от изменений в коде (в нашем случае - файлы в папке src):


```
project: spring
configVersion: 1
---
image: hello
from: maven:3-jdk-8
git:
- add: /
  to: /app
  stageDependencies:
    beforeSetup:
    - "pom.xml"
    install:
    - "src"
mount:
- from: build_dir
  to: /root/.m2/repository
ansible:
  beforeSetup:
  - name: dependency resolve
    shell: |
      mvn -B -f pom.xml dependency:resolve
   args:
     chdir: /app
     executable: /bin/bash
  setup:
  - name: Build jar
    shell: |
      mvn -B -f pom.xml package
    args:
      chdir: /app
      executable: /bin/bash
```


Теперь получили именно то что хотели - при первой сборке без кешей - сборка будет сравнительно долгой. Будут скачиваться репозитории maven, описанные в pom.xml, затем из описанного в src будет пытаться собраться приложение. Напомню о механизмах отладки на этом этапе -  \
`--introspect-before-error  \
--introspect-error` \
И в случае проблем можем попасть внутрь контейнера и выполнить команды сборки вручную. \
Можем уже в таком виде положить наш итоговый образ в registry. Но базовый образ maven:3-jdk-8 достаточно тяжелый, наше собраннное приложение куда меньше. В документации spring запускают jar внутри alpine-образа openjdk. Он существенно меньше и достаточен для запуска jar-файла. Для того чтобы этог сделать в начале секции сборки переименуем image в artifact и назовем этот артефакт build. По сути своей это тот же самый image, который мы собирали выше, но используется он только в качестве промежуточного сборщика и результатом его работы является собранный jar, который мы должны будем отдать в image с alpine-openjdk для пуша в registry и последующего запуска. Опишем эту взаимосвязь и новый image “spring”. После --- который говорит сборщику, что далее идет новый image опишем image: spring - это image базирующийся на openjdk:8-jdk-alpine в котором мы директивой import, из /app/target импортируем jar-файл в папку /app, откуда и будем запускать. Итоговый werf.yaml: \



```
project: spring
configVersion: 1
---
artifact: build
from: maven:3-jdk-8
git:
- add: /
  to: /app
  stageDependencies:
    beforeSetup:
    - "pom.xml"
    install:
    - "src"
mount:
- from: build_dir
  to: /root/.m2/repository
ansible:
  beforeSetup:
  - name: dependency resolve
    shell: |
      mvn -B -f pom.xml dependency:resolve
   args:
     chdir: /app
     executable: /bin/bash
  setup:
  - name: Build jar
    shell: |
      mvn -B -f pom.xml package
    args:
      chdir: /app
      executable: /bin/bash
---
image: hello
from: openjdk:8-jdk-alpine
import:
- artifact: build
  add: /app/target/gs-spring-boot-docker-0.1.0.jar
  to: /app/app.jar
  after: setup

```



# Генерируем и раздаем ассеты

TODO: поправить js \
Генерировать ассеты в нашем примере (java, spring, maven) можно разными способами. Например, в maven есть [плагин](https://github.com/eirslett/frontend-maven-plugin), который позволяет описать сборку ассетов не выходя из Java. Но там есть несколько оговорок про use-case этого плагина:



*   Не предполагается использовать как замена Node для разработчиков фронтенда. Скорее для того чтобы разработчики бекенда могли быстрее включить JS-код в свою сборку.
*   Не предполагается использование на production-окружениях.

 \
Потому хорошим и распространенным выбором будет использовать webpack отдельно. С помощью werf мы также запустим сборку ассетов в отдельном сборочном контейнере, а отдавать их будем с помощью nginx. Разумеется для этого потребуются изменения в описании сборки - werf.yaml, в helm-чартах и в ingress-контроллере. \
Начнем с самого описания ассетов \
============================================== \
Прикрутим к получившемуся приложению генерацию ассетов. Собирать их будем webpack-ом. Что-то сложное делать не будем, опишу основные принципы. \
Добавим папку assets в наш проект. Внутри неё следующая структура: \



```
├── dist
│   └── index.html
├── package.json
├── src
│   └── index.js
└── webpack.config.js
```


index.html: \



```
<!doctype html>
<html>
 <head>
   <title>Getting Started</title>
 </head>
 <body>
   <script src="https://npmcdn.com/axios/dist/axios.min.js"></script>
   <script src="main.js"></script>
 </body>
</html>
```


package.json


```
{
 "scripts": {
   "build": "webpack --config webpack.config.js"
 },
 "devDependencies": {
   "webpack": "^4.20.2",
   "webpack-cli": "^3.1.2"
 }
}
```


index.js


```
window.axios = require('axios');

document.write('<h1>Hello from frontend!</h1>');
console.log('Hi');

axios.get("http://example.org/demo/all")
       .then(response => {
               document.body.append(JSON.stringify(response.data));
       })
       .catch(error => {
               console.log("error", error);
       });
```


webpack.config.js


```
const path = require('path');

module.exports = {
 entry: './src/index.js',
 output: {
   filename: 'main.js',
   path: path.resolve(__dirname, 'dist'),
 },
 externals: ['axios'],
 mode: 'development'
};
```


Понадобятся так же правки в werf.yaml (нужно собирать ассеты) и в чартах (нужно деплоить).

Итоговый werf.yaml: \



```
project: springboot
configVersion: 1
---
artifact: build
from: maven:3-jdk-8
git:
- add: /
 to: /app
 excludePaths:
   - assets
 stageDependencies:
   install:
   - "pom.xml"
   setup:
   - "**/*"
mount:
- from: build_dir
 to: /root/.m2/repository
ansible:
 install:
 - name: resolve dependencies
   shell: |
     mvn -B -f pom.xml dependency:resolve
   args:
     chdir: /app
     executable: /bin/bash
 setup:
 - name: Build jar
   shell: |
     mvn -B -f pom.xml package
   args:
     chdir: /app
     executable: /bin/bash
---
artifact: npm-build
from: node:latest
git:
 - add: /assets
   to: /app
   stageDependencies:
     install:  
     - package.json
     setup:
     - "**/*"
     - "src"
ansible:
 install:
 - name: npm install
   shell: |
     npm install
   args:
     chdir: /app
     executable: /bin/bash

 setup:
 - name: npm build
   shell: |
     npm run build
   args:
     chdir: /app
     executable: /bin/bash
---
image: springboot
from: openjdk:8-jdk-alpine
import:
- artifact: build
 add: /app/target/gs-spring-boot-docker-0.1.0.jar
 to: /app/target/gs-spring-boot-docker-0.1.0.jar
 after: setup
---
image: frontend
from: nginx:alpine
import:
- artifact: npm-build
 add: /app/
 to: /app/
 after: install
```


Добавился еще 1 артефакт для npm build и итоговый образ с nginx. nginx-у еще потребуется подсовывать конфиг для того чтобы он мог отдавать файлы из /app. Заодно передадим в env.js, который будем инклюдить для примера использования env для разных окружений. Делаем это, чтобы образ для dev/stage/prod-окружений был одним и тем же. А всю разницу, что возникает из-за разных окружений мы закрываем переменными  \
Добавим:

Общий блок, наверное. Но у рельсов вроде как есть обёртка над вебпаком.

ВАЖНО отразить применение конфигов. Где мы задаём переменные окружения, что не надо их пихать в стадию сборки (как делают некоторые фронтэндеры например) и т.п. Если поведение собранного франта надо параметризовать в зависимости от окружения, нужно в бекенде сделать генерацию малюсенького js файла с переменными, который инклюдится на каждой странице.

=====================================================================


# Работа с файлами и электронной почтой

Файлы – варианта два:



1. У вас в кубе есть сетефая файловая система (EFS, NFS, …), которая позволяет подключить общую директорию ко многим подам одновременно. Тогда можно работать по-старинке.
2. Правильный вариант – S3.

Почта – один вариант, используем внешнее API.


# Подключаем redis

Допустим к нашему приложению нужно подключить простейшую базу данных, например, redis или memcached. Возьмем первый вариант.

В простейшем случае нет необходимости вносить изменения в сборку — всё уже собрано для нас. Надо просто подключить нужный образ, а потом в вашем Java-приложении корректно обратиться к этому приложению.


## Завести Redis в Kubernetes

Есть два способа подключить: прописать helm-чарт самостоятельно или подключить внешний. Мы рассмотрим второй вариант.

Подключим redis как внешний subchart.

Для этого нужно:



1. прописать изменения в yaml файлы; 
2. указать редису конфиги
3. подсказать werf, что ему нужно подтягивать subchart.

Добавим в файл .helm/requirements.yaml следующие изменения:


```
dependencies:
- name: redis
  version: 9.3.2
  repository: https://kubernetes-charts.storage.googleapis.com/
  condition: redis.enabled
```


Для того чтобы werf при деплое загрузил необходимые нам сабчарты - нужно добавить команды в .gitlab-ci


```
.base_deploy:
  stage: deploy
  script:
    - werf helm repo init
    - werf helm dependency update
    - werf deploy
```


Опишем параметры для redis в файле `.helm/values.yaml`


```
redis:
  enabled: true
```


При использовании сабчарта по умолчанию создается master-slave кластер redis. 

Если посмотреть на рендер (werf helm render) нашего приложения с включенным сабчартом для redis, то можем увидеть какие будут созданы сервисы:


```
# Source: example-2/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-master

# Source: example-2/charts/redis/templates/redis-slave-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-slave
```



## Подключение Java приложения к базе redis

В нашем приложении - мы будем  подключаться к мастер узлу редиса. Нам нужно, чтобы при выкате в любое окружение приложение подключалось к правильному редису.

Рассмотрим настройки подключения к redis из нашего приложения.

Тут дальше описываем как к какому порту подрубиться, чтобы подключиться. Возможно даже — куда мы и что пишем в приложении, чтобы это произошло.


# Подключаем базу данных

Тут будут единые примеры, которые во всех статьях используются. Ибо для сборки и деплоя как-то не сильно разно получается, и хельм одинаковый.


## Как подключить БД \
 \
Рассмотрим тот же spring-boot, но уже с mysql. За основу возьмем [https://spring.io/guides/gs/accessing-data-mysql/](https://spring.io/guides/gs/accessing-data-mysql/). Возьмем за основу тот же werf.yaml, что использовался выше, только поменяем имя генерируемого jar для соответствия коду с сайта. 

Единственное отличие с инструкцией spring будет в файле `src/main/resources/application.properties - `будем брать хост, пароль, бд и имя пользователя из переменных окружения:` \
`


```
spring.datasource.url=jdbc:mysql://${MYSQL_HOST:localhost}:3306/${MYSQL_DATABASE}
spring.datasource.username=${MYSQL_USER}
spring.datasource.password=${MYSQL_PASSWORD}
```


` \
`Пропишем их в values.yaml: \
 \



```
infr:
 mysql:
   host:
     _default: mysql
   db:
     _default: db_example
   user:
     _default: springuser
   password:
     _default: ThePassword
   rootpassword:
     _default: root
```


` \
`Также нам понадобится сам mysql. Опишем его рядом - в helm-чарте: \
 \



```
apiVersion: apps/v1
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
 name: mysql
spec:
 selector:
   matchLabels:
     service: mysql
 replicas: 1
 serviceName: mysql
 template:
   metadata:
     labels:
       service: mysql
   spec:
     containers:
     - name: mysql
       image: mysql:8.0.16
       args:
       - --default-authentication-plugin=mysql_native_password
       env:
       - name: MYSQL_DATABASE
         value: {{ pluck .Values.global.env .Values.infr.mysql.db | first | default .Values.infr.mysql.db._default }}
       - name: MYSQL_PASSWORD
         value: {{ pluck .Values.global.env .Values.infr.mysql.password | first | default .Values.infr.mysql.password._default }}
       - name: MYSQL_ROOT_PASSWORD
         value: root
       - name: MYSQL_USER
         value: {{ pluck .Values.global.env .Values.infr.mysql.user | first | default .Values.infr.mysql.user._default }}
       ports:
       - containerPort: 3306
       volumeMounts:
       - mountPath: /var/lib/mysql
         name: data
     volumes:
     - name: data
       emptyDir: {}
     restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
 name: mysql
 labels:
   service: mysql
spec:
 ports:
 - name: "3306"
   port: 3306
 selector:
   service: mysql
```


Запуск приложения зависит от наличия mysql, добавим init-container для приложения, чтобы приложение не запускалось до появления коннекта к mysql. 

добавится следующая секция в 10-app.yml: \



```
    spec:
     initContainers:
     - name: wait-mysql
       image: alpine:3.9
       command:
       - /bin/sh
       - -c
       - while ! getent ahostsv4 $MYSQL_HOST; do echo waiting for mysql; sleep 2; done
```


Собираем приложение аналогично первому. Проверяем: \
`curl 172.17.0.7:8080/demo/all`


```
[]
curl 172.17.0.7:8080/demo/add -d name=First -d email=someemail@someemailprovider.com
Saved
curl 172.17.0.7:8080/demo/add -d name=Second -d email=some2email@someemailprovider.com     
Saved
curl 172.17.0.7:8080/demo/all |jq
[
 {
   "id": 1,
   "name": "First",
   "email": "someemail@someemailprovider.com"
 },
 {
   "id": 2,
   "name": "Second",
   "email": "some2email@someemailprovider.com"
 }
]
```



## Выполнение миграций

TODO:  \
 \
[https://www.liquibase.org/get_started/quickstart_sql.html](https://www.liquibase.org/get_started/quickstart_sql.html)


## Накатка фикстур при первом выкате

При использовании spring фреймворк сам следит за наличием нужного в БД и при пустой БД создает все что нужно сам. При наличии прав, разумеется. Разработчики рекомендуют отбирать подобные права у пользователя spring после изначальной инициализации приложения.


# Юнит-тесты и Линтеры

[https://spring.io/guides/gs/testing-web/](https://spring.io/guides/gs/testing-web/) unit-test \
[https://spring.io/guides/gs/maven/](https://spring.io/guides/gs/maven/) unit-test

[https://maven.apache.org/plugins/maven-checkstyle-plugin/usage.html](https://maven.apache.org/plugins/maven-checkstyle-plugin/usage.html) lint

Юниты выполняются при сборк, другие типы тестов могут и должны выполняться на других стадиях. Пока это в доке отстутствует, будет в будущем.


# Несколько приложений в одной репе



1. Добавляем кронджоб
2. Добавляем воркер/консюмер
3. Добавляем вторую приложуху на другом языке (например, это может быть webscoket’ы на nodejs; показать организацию helm, организацию werf.yaml, и ссылку на другую статью). Генерация ассетов подойдет?


# Динамические окружения

TODO: 
