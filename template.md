---
author_team: "bravo"
author_name: "Кот Васька"
ci: "gitlab"
language: "python"
framework: "flask"
is_compiled: 0
package_managers_possible:
 - pip
package_managers_chosen: "pip"
unit_tests_possible:
 - flask-sqlalchemy
 - pytest
 - unittest
 - nose
 - nose2
unit_tests_chosen: "flask-sqlalchemy"
assets_generator_possible:
 - webpack
 - gulp
assets_generator_chosen: "webpack"
---

# Чек-лист готовности статьи
<ol>
<li>Все примеры кладём в <a href="https://github.com/flant/examples">https://github.com/flant/examples</a>

<li>Для каждой статьи может и должно быть НЕСКОЛЬКО примеров, условно говоря — по примеру на главу это нормально.

<li>Делаем примеры И на Dockerfile, И на Stapel

<li>Про хельм говорим, про особенности говорим, но в подробности не вдаёмся — считаем, что человек умеет в кубовые ямлы.

<li>Обязательно тестируйте свои примеры перед публикацией
</li>
</ol>

# Введение

Рассмотрим разные способы которые помогут {{Python программисту собрать приложение на Flask}} и запустить его в kubernetes кластере.

Предполагается что читатель имеет базовые знания в разработке на {{Python и Flask}} а также немного знаком с Gitlab CI и примитивами kubernetes, либо готов во всём этом разобраться самостоятельно. Мы постараемся предоставить все ссылки на необходимые ресурсы, если потребуется приобрести какие то новые знания.  

Собирать приложения будем с помощью werf. Данный инструмент работает в Linux MacOS и Windows, инструкция по [установке](https://ru.werf.io/documentation/guides/installation.html) находится на официальном [сайте](https://ru.werf.io/). В качестве примера - также приложим Docker файлы.

Для иллюстрации действий в данной статье - создан репозиторий с исходным кодом, в котором находятся несколько простых приложений. Мы постараемся подготовить примеры чтобы они запускались на вашем стенде и постараемся подсказать, как отлаживать возможные проблемы при вашей самостоятельной работе.


## Подготовка приложения

Наилучшим образом приложения будут работать в Kubernetes - если они соответствуют [12 факторам heroku](https://12factor.net/). Благодаря этому - у нас в kubernetes работают stateless приложения, которые не зависят от среды. Это важно, так как кластер может самостоятельно переносить приложения с одного узла на другой, заниматься масштабированием и т.п. — и мы не указываем, где конкретно запускать приложение, а лишь формируем правила, на основании которого кластер принимает свои собственные решения.

Договоримся что наши приложения соответствуют этим требованиям. На хабре уже было описание данного подхода, вы можете почитать про него например [тут](https://12factor.net/).


## Подготовка и настройка среды

Для того, чтобы пройти по этому гайду, необходимо, чтобы

*   У вас был работающий и настроенный Kubernetes кластер
*   Код приложения находился в Gitlab
*   Был настроен Gitlab CI, подняты и подключены к нему раннеры

Для пользователя под которым будет производиться запуск runner-а - нужно установить multiwerf - данная утилита позволяет переключаться между версиями werf и автоматически обновлять его. Инструкция по установке - доступна по [ссылке](https://ru.werf.io/documentation/guides/installation.html#installing-multiwerf).

Для автоматического выбора актуальной версии werf в канале stable, релиз 1.1 выполним следующую  команду:

```
. $(multiwerf use 1.1 stable --as-file)
```

Перед деплоем нашего приложения необходимо убедиться что у нас подготовлены инфраструктурные компоненты:

*   К gitlab подключен shell runner с тегом werf. [Инструкция](https://ru.werf.io/documentation/guides/gitlab_ci_cd_integration.html#%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-runner) по подготовке gitlab runner.
*   Ранеры включены и активны для репозитория с нашим приложением
*   Для пользователя под которым запускается сборка и деплой установлен kubectl и добавлен конфигурационный файл для подключения к kubernetes.
*   Для gitlab включен и настроен gitlab registry
*   Раннер запущен на отдельной виртуалке, имеет доступ к API kubernetes и запускается по тегу werf  

## Настройка Gitlab Runner

TODO: наверное надо это сюда в шаблон вписать

# Hello world

В первой главе мы покажем поэтапную сборку и деплой приложения без задействования внешних ресурсов таких как база данных и сборку ассетов.

Наше приложение будет состоять из одного docker образа собранного с помощью werf. Его единственной задачей будет вывод сообщения “hello world” по http.

{{В этом образе будет работать один основной процесс gunicorn, который запустит приложение через wsgi.}} 

Управлять маршрутизацией запросов к приложению будет управлять Ingress в kubernetes кластере.

Мы реализуем два стенда: production и staging. В рамках hello world приложения мы предполагаем, что разработка ведётся локально, на вашем компьютере.

_В ближайшее время werf реализует удобные инструменты для локальной разработки, следите за обновлениями._


## Локальная сборка

{{Где мы берём исходники приложения}}

Для того чтобы werf смогла начать работу с нашим приложением - необходимо в корне нашего репозитория создать файл werf.yaml в которым будут описаны инструкции по сборке. Для начала соберем образ локально не загружая его в registry чтобы разобраться с синтаксисом сборки. 

С помощью werf можно собирать образы с используя Dockerfile или используя синтаксис, описанный в документации werf (мы называем этот синтаксис и движок, который этот синтаксис обрабатывает, stapel). Для лучшего погружения - соберем наш образ с помощью stapel.

Итак, начнём с самой главной секции нашего werf.yaml файла, которая должна присутствовать в нём **всегда**. Называется она [meta config section](https://werf.io/documentation/configuration/introduction.html#meta-config-section) и содержит всего два параметра.

werf.yaml:
```
project: {{chat}}
configVersion: 1
```

**_project_** - поле, задающее имя для проекта, которым мы определяем связь всех docker images собираемых в данном проекте. Данное имя по умолчанию используется в имени helm релиза и имени namespace в которое будет выкатываться наше приложение. Данное имя не рекомендуется изменять (или подходить к таким изменениям с должным уровнем ответственности) так как после изменений уже имеющиеся ресурсы, которые выкачаны в кластер, не будут переименованы.

**_configVersion_** - в данном случае определяет версию синтаксиса используемую в `werf.yaml`.

После мы сразу переходим к следующей секции конфигурации, которая и будет для нас основной секцией для сборки - [image config section](https://werf.io/documentation/configuration/introduction.html#image-config-section). И чтобы werf понял что мы к ней перешли разделяем секции с помощью тройной черты.


```
project: chat
configVersion: 1
---
{{image: jopa}}
{{from: jopa:14-01-03}}
```

{{Блок image и связанное с ним}}

Теперь встает вопрос о том как нам добавить наш исходный код внутрь нашего docker image. И для этого мы можем использовать Git! И нам даже не придётся устанавливать его внутрь docker image.

**_git_**, на наш взгляд это самый правильный способ добавления ваших исходников внутрь docker image, хотя существуют и другие. Его преимущество в том что он именно клонирует, и в дальнейшем накатывает коммитами изменения в тот исходный код что мы добавили внутрь нашего docker image, а не просто копирует файлы. Вскоре мы узнаем зачем это нужно.

```
project: chat
configVersion: 1
---
{{image: jopa}}
{{from: jopa:14-01-03}}
git:
- add: /
  to: /app
```

Werf подразумевает что ваша сборка будет происходить внутри директории склонированного git репозитория. Потому мы списком можем указывать директории и файлы относительно корня репозитория которые нам нужно добавить внутрь image.

`add: /` - та директория которую мы хотим добавить внутрь docker image, мы указываем, что это весь наш репозиторий

`to: /app` - то куда мы клонируем наш репозиторий внутри docker image. Важно заметить что директорию назначения werf создаст сам.

 Есть возможность даже добавлять внешние репозитории внутрь проекта не прибегая к предварительному клонированию, как это сделать можно узнать [тут](https://werf.io/documentation/configuration/stapel_image/git_directive.html), но мы не рекомендуем такой подход.

{{Блок ansible и вот это всё, что в него входит:}}
{{*   Кратко объяснить, про стадии, но объяснить, что подробнее мы будем смотреть позже. Пока что обозначаем одну-две стадии и говорим, что вот они такие.}}
{{*   Про то, как мы дёргаем ансибл и что-то доустанавливаем.}}
{{*   ЕСЛИ есть компиляция - вот тут будем её показывать}}

Полный список поддерживаемых модулей ansible в werf можно найти [тут](https://werf.io/documentation/configuration/stapel_image/assembly_instructions.html#supported-modules).

Не забыв [установить werf](https://werf.io/documentation/guides/installation.html) локально, запускаем сборку с помощью [werf build](https://werf.io/documentation/cli/main/build.html)!

```
$  werf build --stages-storage :local
```

![alt_text](images/-0.gif "image_tooltip")

Вот и всё, наша сборка успешно завершилась. К слову если сборка падает и вы хотите изнутри контейнера её подебажить вручную, то вы можете добавить в команду сборки флаги:

```
--introspect-before-error
```

или

```
--introspect-error
``` 

Которые при падении сборки на одном из шагов автоматически откроют вам shell в контейнер, перед исполнением проблемной инструкции или после.

В конце werf отдал информацию о готовом image:

![alt_text](images/-1.png "image_tooltip")

Теперь его можно запустить локально используя image_id просто с помощью docker.
Либо вместо этого использовать [werf run](https://werf.io/documentation/cli/main/run.html):


```
werf run --stages-storage :local --docker-options="-d -p 8080:8080 --restart=always" -- {{node /app/src/js/index.js}}
```

Первая часть команды очень похожа на build, а во второй мы задаем [параметры](https://docs.docker.com/engine/reference/run/) docker и через двойную черту команду с которой хотим запустить наш image.

Небольшое пояснение про `--stages-storage :local `который мы использовали и при сборке и при запуске приложения. Данный параметр указывает на то где werf хранить стадии сборки. На момент написания статьи это возможно только локально, но в ближайшее время появится возможность сохранять их в registry.

Теперь наше приложение доступно локально на порту 8080:

![alt_text](images/-2.png "image_tooltip")

На этом часть с локальным использованием werf мы завершаем и переходим к той части для которой werf создавался, использовании его в CI.

## Построение CI-процесса

После того как мы закончили со сборкой, которую можно производить локально, мы приступаем к базовой настройке CI/CD на базе Gitlab.

Начнем с того что добавим нашу сборку в CI с помощью .gitlab-ci.yml, который находится внутри корня проекта. Нюансы настройки CI в Gitlab можно найти [тут](https://docs.gitlab.com/ee/ci/).

Мы предлагаем простой флоу, который мы называем [fast and furious](#). Такой флоу позволит вам осуществлять быструю доставку ваших изменений в production согласно методологии GitOps и будут содержать два окружения, production и stage.

На стадии сборки мы будем собирать образ с помощью werf и загружать образ в registry, а затем на стадии деплоя собрать инструкции для kubernetes, чтобы он скачивал нужные образы и запускал их.

### Сборка в Gitlab CI

Для того, чтобы настроить CI-процесс создадим .gitlab-ci.yaml в корне репозитория с таким содержанием:

{{Версию верфи выносим в переменную}}

{{Сборку стадий прописываем в WERF_STAGES_STORAGE}}

{{Выносим подключение верфи в before_script (вот эти type multiwerf и type werf)}}

{{В самой билд стадии делаем werf build-and-publish}}

{{Если есть компиляция — описываем как её делать}}

{{Про content-based tagging}}

{{Отдельно проговариваем историю с проброской конфигов в стадию сборки.}} 

Теперь мы можем запушить наши изменения и увидеть что наша стадия успешно выполнилась.

![alt_text](images/-3.png "image_tooltip")


Лог в Gitlab будет выглядеть так же как и при локальной сборке, за исключением того что в конце мы увидим как werf пушит наш docker image в registry.

```
207 │ ┌ Publishing image {{node}} by stages-signature tag c905b748cb9647a03476893941837bf79910ab09e ...
208 │ ├ Info
209 │ │   images-repo: registry.gitlab-example.com/{{chat/node}}
210 │ │        image: registry.gitlab-example.com/{{chat/node}}:c905b748cb9647a03476893941 ↵
211 │ │   837bf79910ab09ef5878037592a45d
212 │ └ Publishing image {{node}} by stages-signature tag c905b748cb9647a0347689394 ... (14.90 seconds)
213 └ ⛵ image {{node}} (73.44 seconds)
214 Running time 73.47 seconds
218 Job succeeded
```

### Деплой

werf использует встроенный Helm для применения конфигурации в Kubernetes. Для описания объектов Kubernetes werf использует конфигурационные файлы Helm: шаблоны и файлы с параметрами (например, values.yaml). Помимо этого, werf поддерживает дополнительные файлы, такие как файлы c секретами и с секретными значениями (например secret-values.yaml), а также дополнительные Go-шаблоны для интеграции собранных образов.

Werf (по аналогии с helm) берет yaml шаблоны, генерирует из них  огромную простыню с финальными ямлами, куда подставлены все значения. В этой простыне ямла — аннотации для кубернетеса. Эта простыня закидывается в кубернетес кластер, который парсит инструкции в ямле и вносит изменения в кластер. Верфь смотрит за тем, как кубернетес вносит изменения и дожидается, чтобы реально всё было применено.

#### Общее про хельм-конфиги

На сегодняшний день [Helm](https://helm.sh/) один из самых удобных способов которым вы можете описать свой deploy в Kubernetes. Кроме возможности установки готовых чартов с приложениями прямиком из репозитория, где вы можете введя одну команду, развернуть себе готовый Redis, Postgres, Rabbitmq прямиком в Kubernetes, вы также можете использовать Helm для разработки собственных чартов с удобным синтаксисом для шаблонизации выката ваших приложений.

Потому для werf это был очевидный выбор использовать такую технологию.

Мы не будем вдаваться в подробности разработки yaml манифестов с помощью Helm для Kubernetes. Осветим лишь отдельные её части, которые касаются данного приложения и werf в целом. Если у вас есть вопросы о том как именно описываются объекты Kubernetes, советуем посетить страницы документации по Kubernetes с его [концептами](https://kubernetes.io/ru/docs/concepts/) и страницы документации по разработке [шаблонов](https://helm.sh/docs/chart_template_guide/) в Helm.

Нам понадобятся следующие файлы со структурой каталогов:


```
.helm (здесь мы будем описывать деплой)
├── templates (объекты kubernetes в виде шаблонов)
│   ├── deployment.yaml (основное приложение)
│   ├── ingress.yaml (описание для ingress)
│   └── service.yaml (сервис для приложения)
├── secret-values.yaml (файл с секретными переменными)
└── values.yaml (файл с переменными для параметризации шаблонов)
```

Подробнее читайте в [нашей статье](https://habr.com/ru/company/flant/blog/423239/) из серии про Helm.

![alt_text](images/-4.png "image_tooltip")

#### Описание приложения в хельме

{{Рассказываем что мы вообще по логике закатывать в хельм.}}

{{Про проброску конфигов через переменные окружения и почему это так.}}

{{Верфёвые команды в хельмах. как оно что оно}}

{{ОБЯЗАТЕЛЬНО написать про то, что логи в stdout и как это сделать в этом фреймворке (не исключает использования Sentry-подобных штук, о чём будет позже)}}

#### Роутинг

{{ОБЯЗАТЕЛЬНО отобразить историю с роутингом на приложение (ингрессы, вот это всё) и кратко про ебель с отладкой этого говна}}

{{Проброска ci_url и какова логика}}

#### Секретные переменные

{{У верфи есть эта штука, её надо описать}}

#### Деплой в Gitlab CI

Далее нам нужно описать стадии выката.

В общей сложности они у нас будут отличаться только параметрами, потому мы напишем для них небольшой шаблон:

```
.base_deploy: &base_deploy
  script:
    - type multiwerf && source <(multiwerf use ${WERF_VERSION})
    - type werf && source <(werf ci-env gitlab --tagging-strategy stages-signature --verbose)
    - werf deploy --stages-storage :local
  dependencies:
    - Build
  tags:
    - article-werf
```

Скрипт стадий выката отличается от сборки всего одной командой:

```
    - werf deploy --stages-storage :local
```

И тут назревает вполне логичный вопрос - а как?

Как werf понимает куда нужно будет деплоить и каким образом? На это есть два ответа.

Первый из них вы уже видели и заключается он в команде `werf ci-env` которая берёт нужные переменные прямиком из pipeline Gitlab - и в данном случае ту что касается названия окружения.
А о втором мы поговорим чуть ниже после того как добавим сами стадии деплоя в `.gitlab-ci.yml`

```
Deploy to Stage:
  extends: .base_deploy
  stage: deploy
  environment:
    name: stage
  except:
    - schedules
  only:
    - merge_requests
  when: manual

Deploy to Production:
  extends: .base_deploy
  stage: deploy
  environment:
    name: production
  except:
    - schedules
  only:
    - master
```

Описание деплоя содержит в себе немного. Скрипт, указание принадлежности к стадии **deploy**, которую мы описывали в начале gitlab-ci.yml, и **dependencies** что означает что стадия не может быть запущена без успешного завершения стадии **Build**. Также мы указали с помощью **only**, ветку _master_, что означает что стадия будет доступна только из этой ветки. **environment** указали потому что werf необходимо понимать в каком окружении он работает. В дальнейшем мы покажем, как создать CI для нескольких окружений. Остальные параметры вам уже известны.

Теперь обязательно на сервере с gitlab-runner, который занимается сборкой вашего приложения, установить [kubectl](https://kubernetes.io/ru/docs/tasks/tools/install-kubectl/). И положить в домашнюю директорию пользователя gitlab-runner конфиг kubernetes в папку `.kube` (нужно её создать, если её нет)

Конфиг можно взять на мастере кластера kubernetes в `/etc/kubernetes/admin.conf`

Скопируйте его и положите в папку `.kube` переименовав файл в `config`.

![alt_text](images/-5.png "image_tooltip")

Обычно в конфиге указан прямой адрес мастера kubernetes. Соответственно нужно чтобы мастер нашего кластера был доступен по сети для gitlab-runner.

Всё это мы сделали для того чтобы werf мог общаться с API kubernetes и деплоить в него наши приложения, это и есть второй ответ на вопрос “Как werf понимает куда ему нужно деплоить?”. По умолчанию деплой будет происходить в namespace состоящий из имени проекта задаваемого в `werf.yaml` и окружения задаваемого в `.gitlab-ci.yml` куда мы деплоим наше приложение.

Ну а теперь достаточно создать Merge Request и нам будет доступна кнопка Deploy to Stage.

![alt_text](images/-6.png "image_tooltip")

Посмотреть статус выполнения pipeline можно в интерфейсе gitlab **CI / CD - Pipelines**

![alt_text](images/-7.png "image_tooltip")


Список всех окружений - доступен в меню **Operations - Environments**

![alt_text](images/-8.png "image_tooltip")

Из этого меню - можно так же быстро открыть приложение в браузере.

{{И тут в итоге должна быть картинка как аппка задеплоилась и объяснение картинки}}

# Подключаем зависимости

{{Что мы используем в качестве менеджера зависимостей в этой экосистеме.}}

{{Где оно хранит свои зависимости}}

{{Как происходит разворачивание зависимостей (какая-то команда в стиле npm install // composer install // …)}}

{{Нам не надо вызывать пересборку зависимостей на каждом коммите, но когда меняется вот этот конкретный файл — надо делать пересборку.}}

{{А ещё у этой штуки есть кэш, чтобы каждый раз менеджер зависимостей не тягал вообще всё каждый раз, он лежит в такой-то папке.}}

{{Для того, чтобы оптимизировать первое — юзаем стадии.}}

{{Рассказываем более глубоко про стадии и выбираем правильную.}}

{{А для того, чтобы с кэшем локальным работать есть артефакты, монтировать вот так-то.}}

# Генерируем и раздаем ассеты

{{Что мы берём в качестве генерации ассетов.}} 

{{Есть три вопроса связанных с генерацией ассетов:}}

{{*   Как описывается сценарий сборки ассетов}}

{{*   Как может конфигурироваться этот сценарий (какие конфиги зависят от окружения, какие — нет, как отличить одно от другого)}}

{{*   Куда складываются ассеты, в какую папку. И как потом ассеты раздаются.}}

{{    *   Какие изменения в сборку}}

{{    *   Какие изменения в делпой}}

{{    *   Какие изменения в ингрессе}}

## Какой сценарий сборки ассетов

{{Как происходит сборка ассетов, описан ли где-то этот процесс в виде кода? Или он прошит во фреймворке и нужно просто следовать каким-то правилам, и если это так — то где прочитать эти правила?}}

## Как конфигурируем сценарий сборки?

{{Про то, что есть два типа опций при сборке:}}

{{*   Те, что будут зависеть от окружения. И привести примеры таких вещей для вашего случая (урлики какие-нибудь в конфигах). }}

{{И объясняем, как мы подходим к конфигурированию таких вещей концептуально.}}

{{*   Те, что не зависят от окружения. И привести пример для вашего случая (например, это бывает волшебный хэшик для обхода кэша браузера) }}

{{И объясняем, как мы подходим к конфигурированию таких вещей концептуально.}}

{{*   Что-то ещё???}}

## Какие изменения необходимо внести

{{Концептуальное описание того, что у нас теперь два разных пода, два разных контейнера и как мы будем распределять между ними запросы и вот это всё.}}

### Изменения в сборке

{{Какие вносим изменения в стадию сборки. Подробности о том, как конкретно пробрасываем конфиги в стадию сборки.}}

### Изменения в деплое

{{Какие вносим изменения в стадию деплоя.}}

### Изменения в роутинге

{{Про ингрессы коротко.}}

# Работа с файлами и электронной почтой

{{Файлы – КАЗАЛОСЬ БЫ:}}

{{У вас в кубе есть сетевая файловая система (EFS, NFS, …), которая позволяет подключить общую директорию ко многим подам одновременно. Тогда можно работать по-старинке.}}

{{НО ЭТО НЕ ПРАВИЛЬНЫЙ ВАРИАНТ}}

{{Правильный вариант – S3.}}

{{Почта – один вариант, используем внешнее API.}}

# Подключаем redis

Допустим к нашему приложению нужно подключить простейшую базу данных, например, redis или memcached. Возьмем первый вариант.

В простейшем случае нет необходимости вносить изменения в сборку — всё уже собрано для нас. Надо просто подключить нужный образ, а потом в вашем {{Python}} приложении корректно обратиться к этому приложению.

## Завести Redis в Kubernetes

Есть два способа подключить: прописать helm-чарт самостоятельно или подключить внешний. Мы рассмотрим второй вариант.

Подключим redis как внешний subchart.

Для этого нужно:

1. прописать изменения в yaml файлы; 
2. указать редису конфиги
3. подсказать werf, что ему нужно подтягивать subchart.

Добавим в файл `.helm/requirements.yaml` следующие изменения:

```
dependencies:
- name: redis
  version: 9.3.2
  repository: https://kubernetes-charts.storage.googleapis.com/
  condition: redis.enabled
```

Для того чтобы werf при деплое загрузил необходимые нам сабчарты - нужно добавить команды в `.gitlab-ci`

```
.base_deploy:
  stage: deploy
  script:
    - werf helm repo init
    - werf helm dependency update
    - werf deploy
```

Опишем параметры для redis в файле `.helm/values.yaml`

```
redis:
  enabled: true
```

При использовании сабчарта по умолчанию создается master-slave кластер redis. 

Если посмотреть на рендер (`werf helm render`) нашего приложения с включенным сабчартом для redis, то можем увидеть какие будут созданы сервисы:

```
# Source: example-2/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-master

# Source: example-2/charts/redis/templates/redis-slave-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-slave
```

## Подключение {{Python}} приложения к базе redis

В нашем приложении - мы будем  подключаться к мастер узлу редиса. Нам нужно, чтобы при выкате в любое окружение приложение подключалось к правильному редису.

{{Рассмотрим настройки подключения к redis из нашего приложения.}}

{{Тут дальше описываем как к какому порту подрубиться, чтобы подключиться. Возможно даже — куда мы и что пишем в приложении, чтобы это произошло.}}

# Подключаем базу данных

{{Для текущего примера в приложении должны быть установлены необходимые зависимости. В качестве примера - мы возьмем приложение для работы которого необходима база данных.}}

## Как подключить БД

{{То же, что в редисе, но просто другая бд, ну вы же не кретины, раз два три}}

## Выполнение миграций

{{Напоминаем, как мы запускаем миграции в этом фреймворке}}

{{Объясняем, куда мы это вписываем вот во всех этих сборках-деплоях и почему именно туда}}

## Накатка фикстур при первом выкате

{{Объясняем, куда это вписывать во всех тих сборках-деплоях.}}


# Юнит-тесты и Линтеры

{{Говорим, что за юнит-тесты/линтеры в этом фреймворке есть?}}

{{Объясняем, куда мы прописываем запуск этих тестов-линтеров и почему именно туда.}}

# Несколько приложений в одной репе

{{1. Добавляем кронджоб}}
{{2. Добавляем воркер/консюмер}}
{{3. Добавляем вторую приложуху на другом языке (например, это может быть webscoket’ы на nodejs; показать организацию helm, организацию werf.yaml, и ссылку на другую статью)}}

# Динамические окружения

{{TODO:}} 
