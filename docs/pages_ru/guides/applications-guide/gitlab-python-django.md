---
author_team: "echo"
author_name: "Храмов Иван"
ci: "gitlab"
language: "python"
framework: "django"
is_compiled: 0
package_managers_possible:
 - pip
package_managers_chosen: "pip"
unit_tests_possible:
 - flask-sqlalchemy
 - pytest
 - unittest
 - nose
 - nose2
unit_tests_chosen: "unittest"
assets_generator_possible:
 - webpack
 - gulp
assets_generator_chosen: "webpack"
---

# Чек-лист готовности статьи
<ol>
<li>Все примеры кладём в <a href="https://github.com/flant/examples">https://github.com/flant/examples</a>

<li>Для каждой статьи может и должно быть НЕСКОЛЬКО примеров, условно говоря — по примеру на главу это нормально.

<li>Делаем примеры И на Dockerfile, И на Stapel

<li>Про хельм говорим, про особенности говорим, но в подробности не вдаёмся — считаем, что человек умеет в кубовые ямлы.

<li>Обязательно тестируйте свои примеры перед публикацией
</li>
</ol>

# Введение

Рассмотрим разные способы которые помогут собрать приложение на Django и запустить его в kubernetes кластере.

Предполагается что читатель имеет базовые знания в разработке на Python и Django а также немного знаком с Gitlab CI и примитивами kubernetes, либо готов во всём этом разобраться самостоятельно. Мы постараемся предоставить все ссылки на необходимые ресурсы, если потребуется приобрести какие то новые знания.  

Собирать приложения будем с помощью werf. Данный инструмент работает в Linux MacOS и Windows, инструкция по [установке](https://ru.werf.io/documentation/guides/installation.html) находится на официальном [сайте](https://ru.werf.io/). В качестве примера - также приложим Docker файлы.

Для иллюстрации действий в данной статье - создан репозиторий с исходным кодом, в котором находятся несколько простых приложений. Мы постараемся подготовить примеры чтобы они запускались на вашем стенде и постараемся подсказать, как отлаживать возможные проблемы при вашей самостоятельной работе.


## Подготовка приложения

Наилучшим образом приложения будут работать в Kubernetes - если они соответствуют [12 факторам heroku](https://12factor.net/). Благодаря этому - у нас в kubernetes работают stateless приложения, которые не зависят от среды. Это важно, так как кластер может самостоятельно переносить приложения с одного узла на другой, заниматься масштабированием и т.п. — и мы не указываем, где конкретно запускать приложение, а лишь формируем правила, на основании которого кластер принимает свои собственные решения.

Договоримся что наши приложения соответствуют этим требованиям. На хабре уже было описание данного подхода, вы можете почитать про него например [тут](https://12factor.net/).


## Подготовка и настройка среды

Для того, чтобы пройти по этому гайду, необходимо, чтобы

*   У вас был работающий и настроенный Kubernetes кластер
*   Код приложения находился в Gitlab
*   Был настроен Gitlab CI, подняты и подключены к нему раннеры

Для пользователя под которым будет производиться запуск runner-а - нужно установить multiwerf - данная утилита позволяет переключаться между версиями werf и автоматически обновлять его. Инструкция по установке - доступна по [ссылке](https://ru.werf.io/documentation/guides/installation.html#installing-multiwerf).

Для автоматического выбора актуальной версии werf в канале stable, релиз 1.1 выполним следующую  команду:

```
. $(multiwerf use 1.1 stable --as-file)
```

Перед деплоем нашего приложения необходимо убедиться что наша инфраструктура готова к тому чтобы использовать werf. Используя [инструкцию](https://ru.werf.io/documentation/guides/gitlab_ci_cd_integration.html#%D0%BD%D0%B0%D1%81%D1%82%D1%80%D0%BE%D0%B9%D0%BA%D0%B0-runner) по подготовке к использованию Werf в Gitlab CI, вам нужно убедиться что все следующие пункты выполнены:

*   Развернут отдельный сервер с сетевой доступностью до мастер ноды Kubernetes.
*   На данном сервере установлен gitlab-runner.
*   Gitlab-runner подключен к нашему Gitlab с тегом werf в режиме shell executor. 
*   Ранеры включены и активны для репозитория с нашим приложением.
*   Для пользователя, которого использует gitlab-runner и под которым запускается сборка и деплой, установлен kubectl и добавлен конфигурационный файл для подключения к kubernetes.
*   Для gitlab включен и настроен gitlab registry
*   Gitlab-runner имеет доступ к API kubernetes и запускается по тегу werf  


# Hello world

В первой главе мы покажем поэтапную сборку и деплой приложения без задействования внешних ресурсов таких как база данных и сборку ассетов.

Наше приложение будет состоять из одного docker образа собранного с помощью werf. Его единственной задачей будет вывод сообщения “hello world” по http.

В этом образе будет работать один основной процесс gunicorn, который запустит приложение через wsgi.

Управлять маршрутизацией запросов к приложению будет управлять Ingress в kubernetes кластере.

Мы реализуем два стенда: production и staging. В рамках hello world приложения мы предполагаем, что разработка ведётся локально, на вашем компьютере.

_В ближайшее время werf реализует удобные инструменты для локальной разработки, следите за обновлениями._


## Локальная сборка

Для того чтобы werf смогла начать работу с нашим приложением - необходимо в корне нашего репозитория создать файл werf.yaml в которым будут описаны инструкции по сборке. Для начала соберем образ локально не загружая его в registry чтобы разобраться с синтаксисом сборки.

С помощью werf можно собирать образы с используя Dockerfile или используя синтаксис, описанный в документации werf (мы называем этот синтаксис и движок, который этот синтаксис обрабатывает, stapel). Для лучшего погружения - соберем наш образ с помощью stapel.
Прежде всего нам необходимо собрать docker image с нашим приложением внутри. 

Клонируем наши исходники любым удобным способом. В нашем случае это:


```
git clone git@gitlab-example.com:article/tools.git
```

После, в корне склоненного проекта, создаём файл `werf.yaml`. Данный файл будет отвечать за сборку вашего приложения и он обязательно должен находиться в корне проекта. Исходный код находится в отдельной директории _django_, в данном случае это сделано просто для удобства, чтобы подразделить исходный код проекта от части связанной со сборкой.

Итак, начнём с самой главной секции нашего werf.yaml файла, которая должна присутствовать в нём **всегда**. Называется она [meta config section](https://werf.io/documentation/configuration/introduction.html#meta-config-section) и содержит всего два параметра.

werf.yaml:
```yaml
project: tools
configVersion: 1
```

**_project_** - поле, задающее имя для проекта, которым мы определяем связь всех docker images собираемых в данном проекте. Данное имя по умолчанию используется в имени helm релиза и имени namespace в которое будет выкатываться наше приложение. Данное имя не рекомендуется изменять (или подходить к таким изменениям с должным уровнем ответственности) так как после изменений уже имеющиеся ресурсы, которые выкачаны в кластер, не будут переименованы.

**_configVersion_** - в данном случае определяет версию синтаксиса используемую в `werf.yaml`.

После мы сразу переходим к следующей секции конфигурации, которая и будет для нас основной секцией для сборки - [image config section](https://werf.io/documentation/configuration/introduction.html#image-config-section). И чтобы werf понял что мы к ней перешли разделяем секции с помощью тройной черты.


```yaml
project: tools
configVersion: 1
---
image: django
from: python:3.6-stretch
```

**_image_** - поле задающее имя нашего docker image, с которым он будет запушен в registry. Должно быть уникально в рамках одного werf-файла.

**_from _** - задает имя базового образа который мы будем использовать при сборке. Задаем мы его точно так же, как бы мы это сделали в dockerfile. В примере используется {{imagename}} {{объяснение почкему такой}} 

Теперь встает вопрос о том как нам добавить исходный код приложения внутрь нашего docker image. И для этого мы можем использовать Git! И нам даже не придётся устанавливать его внутрь docker image.

**_git_**, на наш взгляд это самый правильный способ добавления ваших исходников внутрь docker image, хотя существуют и другие. Его преимущество в том что он именно клонирует, и в дальнейшем накатывает коммитами изменения в тот исходный код что мы добавили внутрь нашего docker image, а не просто копирует файлы. Вскоре мы узнаем зачем это нужно.

```yaml
project: tools
configVersion: 1
---
image: django
from: python:3.6-stretch
git:
- add: /
  to: /app
```

Werf подразумевает что ваша сборка будет происходить внутри директории склонированного git репозитория. Потому мы списком можем указывать директории и файлы относительно корня репозитория которые нам нужно добавить внутрь image.

`add: /` - та директория которую мы хотим добавить внутрь docker image, мы указываем, что это весь наш репозиторий

`to: /app` - то куда мы клонируем наш репозиторий внутри docker image. Важно заметить что директорию назначения werf создаст сам.

 Есть возможность даже добавлять внешние репозитории внутрь проекта не прибегая к предварительному клонированию, как это сделать можно узнать [тут](https://werf.io/documentation/configuration/stapel_image/git_directive.html), но мы не рекомендуем такой подход.

Следующим этапом необходимо описать правила сборки для приложения. Werf позволяет кэшировать сборку образа подобно слоям в docker, только с явным набором инструкций необходимых данном кэше. Этот сборочный этап - называется стадия. Мы рассмотрим более подробно возможности стадий в с следующих главах.

Для текущего приложения опишем 2 стадии в которых сначала устанавливаем необходимые зависимости для возможности сборки приложения а потом - непосредственно собираем приложение.

Команды описывающие сборку можно описывать в ansible формате или shell командами.
Добавим в `werf.yaml` следующий блок используя ansible синтаксис:

```yaml
ansible:
  install:
  - name: Install requirements
    apt:
      name:
      - locales
      update_cache: yes
  - name: Set timezone
    timezone:
      name: "Etc/UTC"
  - name: Generate locale
    locale_gen:
      name: en_US.UTF-8
      state: present
  setup:
  - name: Install python requirements
    pip:
      requirements: /usr/src/app/requirements.txt
      executable: pip3.6
```

Полный список поддерживаемых модулей ansible в werf можно найти [тут](https://werf.io/documentation/configuration/stapel_image/assembly_instructions.html#supported-modules).

Не забыв [установить werf](https://werf.io/documentation/guides/installation.html) локально, запускаем сборку с помощью [werf build](https://werf.io/documentation/cli/main/build.html)!

```bash
$  werf build --stages-storage :local
```

![alt_text](images/-0.gif "image_tooltip")

Вот и всё, наша сборка успешно завершилась. К слову если сборка падает и вы хотите изнутри контейнера её подебажить вручную, то вы можете добавить в команду сборки флаги:

```yaml
--introspect-before-error
```

или

```yaml
--introspect-error
```

Которые при падении сборки на одном из шагов автоматически откроют вам shell в контейнер, перед исполнением проблемной инструкции или после.

В конце werf отдал информацию о готовом image:

![alt_text](images/-1.png "image_tooltip")

Теперь его можно запустить локально используя image_id просто с помощью docker.
Либо вместо этого использовать [werf run](https://werf.io/documentation/cli/main/run.html):


```bash
werf run --stages-storage :local --docker-options="-d -p 8080:8080 --restart=always" -- python manage.py runserver
```

Первая часть команды очень похожа на build, а во второй мы задаем [параметры](https://docs.docker.com/engine/reference/run/) docker и через двойную черту команду с которой хотим запустить наш image.

Небольшое пояснение про `--stages-storage :local `который мы использовали и при сборке и при запуске приложения. Данный параметр указывает на то где werf хранить стадии сборки. На момент написания статьи это возможно только локально, но в ближайшее время появится возможность сохранять их в registry.

Теперь наше приложение доступно локально на порту 8080:

![alt_text](images/-2.png "image_tooltip")

На этом часть с локальным использованием werf мы завершаем и переходим к той части для которой werf создавался, использовании его в CI.

## Построение CI-процесса

После того как мы закончили со сборкой, которую можно производить локально, мы приступаем к базовой настройке CI/CD на базе Gitlab.

Начнем с того что добавим нашу сборку в CI с помощью .gitlab-ci.yml, который находится внутри корня проекта. Нюансы настройки CI в Gitlab можно найти [тут](https://docs.gitlab.com/ee/ci/).

Мы предлагаем простой флоу, который мы называем [fast and furious](https://docs.google.com/document/d/1a8VgQXQ6v7Ht6EJYwV2l4ozyMhy9TaytaQuA9Pt2AbI/edit#). Такой флоу позволит вам осуществлять быструю доставку ваших изменений в production согласно методологии GitOps и будут содержать два окружения, production и stage.

На стадии сборки мы будем собирать образ с помощью werf и загружать образ в registry, а затем на стадии деплоя собрать инструкции для kubernetes, чтобы он скачивал нужные образы и запускал их.

### Сборка в Gitlab CI

Для того, чтобы настроить CI-процесс создадим .gitlab-ci.yaml в корне репозитория.

Инициализируем werf перед запуском основной команды. Это необходимо делать перед каждым использованием werf поэтому мы вынесли в секцию `before_script`
Такой сложный путь с использованием multiwerf нужен для того, чтобы вам не надо было думать про обновление верфи и установке новых версий — вы просто указываете, что используете, например, use 1.1 stable и пребываете в уверенности, что у вас актуальная версия с закрытыми issues.

```yaml
before_script:
  - type multiwerf && source <(multiwerf use 1.1 stable)
  - type werf && source <(werf ci-env gitlab --verbose)
```

`werf ci-env gitlab --verbose` - готовит наш werf для работы в Gitlab, выставляя для этого все необходимые переменные.
Пример переменных автоматически выставляемых этой командой:

```bash
### DOCKER CONFIG
 export DOCKER_CONFIG="/tmp/werf-docker-config-832705503"
 ### STAGES_STORAGE
 export WERF_STAGES_STORAGE="registry.gitlab-example.com/chat/stages"
 ### IMAGES REPO
 export WERF_IMAGES_REPO="registry.gitlab-example.com/chat"
 export WERF_IMAGES_REPO_IMPLEMENTATION="gitlab"
 ### TAGGING
 export WERF_TAG_BY_STAGES_SIGNATURE="true"
 ### DEPLOY
 # export WERF_ENV=""
 export WERF_ADD_ANNOTATION_PROJECT_GIT="project.werf.io/git=https://lab.gitlab-example.com/chat"
 export WERF_ADD_ANNOTATION_CI_COMMIT="ci.werf.io/commit=61368705db8652555bd96e68aadfd2ac423ba263"
 export WERF_ADD_ANNOTATION_GITLAB_CI_PIPELINE_URL="gitlab.ci.werf.io/pipeline-url=https://lab.gitlab-example.com/chat/pipelines/71340"
 export WERF_ADD_ANNOTATION_GITLAB_CI_JOB_URL="gitlab.ci.werf.io/job-url=https://lab.gitlab-example.com/chat/-/jobs/184837"
 ### IMAGE CLEANUP POLICIES
 export WERF_GIT_TAG_STRATEGY_LIMIT="10"
 export WERF_GIT_TAG_STRATEGY_EXPIRY_DAYS="30"
 export WERF_GIT_COMMIT_STRATEGY_LIMIT="50"
 export WERF_GIT_COMMIT_STRATEGY_EXPIRY_DAYS="30"
 export WERF_STAGES_SIGNATURE_STRATEGY_LIMIT="-1"
 export WERF_STAGES_SIGNATURE_STRATEGY_EXPIRY_DAYS="-1"
 ### OTHER
 export WERF_LOG_COLOR_MODE="on"
 export WERF_LOG_PROJECT_DIR="1"
 export WERF_ENABLE_PROCESS_EXTERMINATOR="1"
 export WERF_LOG_TERMINAL_WIDTH="95"
```


Многие из этих переменных интуитивно понятны, и содержат базовую информацию о том где находится проект, где находится его registry, информацию о коммитах. \
Подробную информацию о конфигурации ci-env можно найти [тут](https://werf.io/documentation/reference/plugging_into_cicd/overview.html). От себя лишь хочется добавить, что если вы используете совместно с Gitlab внешний registry (harbor,Docker Registry,Quay etc.), то в команду билда и пуша нужно добавлять его полный адрес (включая путь внутри registry), как это сделать можно узнать [тут](https://werf.io/documentation/cli/main/build_and_publish.html). И так же не забыть первой командой выполнить [docker login](https://docs.docker.com/engine/reference/commandline/login/).

В рамках статьи нам хватит значений выставляемых по умолчанию.

Переменная [WERF_STAGES_STORAGE](https://ru.werf.io/documentation/reference/stages_and_images.html#%D1%85%D1%80%D0%B0%D0%BD%D0%B8%D0%BB%D0%B8%D1%89%D0%B5-%D1%81%D1%82%D0%B0%D0%B4%D0%B8%D0%B9) указывает где werf сохраняет свой кэш (стадии сборки) У werf есть опция распределенной сборки, про которую вы можете прочитать в нашей статье, в текущем примере мы сделаем по-простому и сделаем сборку на одном узле в один момент времени.


```yaml
variables:
    WERF_STAGES_STORAGE: ":local"
```
Дело в том что werf хранит стадии сборки раздельно, как раз для того чтобы мы могли не пересобирать весь образ, а только отдельные его части.

Плюс стадий в том, что они имеют собственный тэг, который представляет собой хэш содержимого нашего образа. Тем самым позволяя полностью избегать не нужных пересборок наших образов. Если вы собираете ваше приложение в разных ветках, и исходный код в них различается только конфигами которые используются для генерации статики на последней стадии. То при сборке образа одинаковые стадии пересобираться не будут, будут использованы уже собранные стадии из соседней ветки. Тем самым мы резко снижаем время доставки кода.

Основная команда на текущий момент - это werf build-and-publish, которая запускает сборку и публикацию в registry на gitlab runner с тегом werf для любой ветки. Путь до registry и другие параметры беруться верфью автоматически их переменных окружения gitlab ci.

```yaml
Build:
  stage: build
  script:
    - werf build-and-publish
  tags:
    - werf
```

Если вы всё правильно сделали и корректно настроен registry и gitlab ci — вы увидите собранный образ в registry. При использовании registry от gitlab — собранный образ можно увидеть через веб-интерфейс гитлаба.

Следующие параметры тем кто работал с гитлаб уже должны быть знакомы.

**_tags_** - нужен для того чтобы выбрать наш раннер, на который мы навесили этот тэг. В данном случае наш gitlab-runner в Gitlab имеет тэг werf

```yaml
  tags:
    - werf
```

Теперь мы можем запушить наши изменения и увидеть что наша стадия успешно выполнилась.

![alt_text](images/-3.png "image_tooltip")


Лог в Gitlab будет выглядеть так же как и при локальной сборке, за исключением того что в конце мы увидим как werf пушит наш docker image в registry.

```
207 │ ┌ Publishing image {{node}} by stages-signature tag c905b748cb9647a03476893941837bf79910ab09e ...
208 │ ├ Info
209 │ │   images-repo: registry.gitlab-example.com/{{chat/node}}
210 │ │        image: registry.gitlab-example.com/{{chat/node}}:c905b748cb9647a03476893941 ↵
211 │ │   837bf79910ab09ef5878037592a45d
212 │ └ Publishing image {{node}} by stages-signature tag c905b748cb9647a0347689394 ... (14.90 seconds)
213 └ ⛵ image {{node}} (73.44 seconds)
214 Running time 73.47 seconds
218 Job succeeded
```
По умолчанию у werf выставлена стартегия тэгирования docker images `stages-signature`, которая тэгирует ваши docker images на основе контента, который они содержат. Это называется [content based tagging](https://werf.io/documentation/reference/publish_process.html#content-based-tagging). Суть его в том что тэг является хэшсуммой всех стадий сборки. Такой подход избавляет нас от лишних пересборок.
### Деплой в Kubernetes

Werf использует встроенный Helm для применения конфигурации в Kubernetes. Для описания объектов Kubernetes werf использует конфигурационные файлы Helm: шаблоны и файлы с параметрами (например, values.yaml). Помимо этого, werf поддерживает дополнительные файлы, такие как файлы c секретами и с секретными значениями (например secret-values.yaml), а также дополнительные Go-шаблоны для интеграции собранных образов.

Werf (по аналогии с helm) берет yaml шаблоны, которые описывают объекты Kubernetes, и генерирует из них общий манифест. Манифест отдается API Kubernetes, который на его основе внесет все необходимые изменения в кластер. Werf отслеживает как Kubernetes вносит изменения и сигнализирует о результатах в реальном времени. Все это благодаря встроенной в werf библиотеке [kubedog](https://github.com/flant/kubedog).

Внутри Werf доступны команды для работы с Helm, например можно проверить как сгенерируется общий манифест в результате работы werf с шаблонами:

```bash
$ werf helm render
```

Аналогично, доступны команды [helm list](https://werf.io/documentation/cli/management/helm/list.html) и другие.

#### Общее про хельм-конфиги

На сегодняшний день [Helm](https://helm.sh/) один из самых удобных способов которым вы можете описать свой deploy в Kubernetes. Кроме возможности установки готовых чартов с приложениями прямиком из репозитория, где вы можете введя одну команду, развернуть себе готовый Redis, Postgres, Rabbitmq прямиком в Kubernetes, вы также можете использовать Helm для разработки собственных чартов с удобным синтаксисом для шаблонизации выката ваших приложений.

Потому для werf это был очевидный выбор использовать такую технологию.

Мы не будем вдаваться в подробности разработки yaml манифестов с помощью Helm для Kubernetes. Осветим лишь отдельные её части, которые касаются данного приложения и werf в целом. Если у вас есть вопросы о том как именно описываются объекты Kubernetes, советуем посетить страницы документации по Kubernetes с его [концептами](https://kubernetes.io/ru/docs/concepts/) и страницы документации по разработке [шаблонов](https://helm.sh/docs/chart_template_guide/) в Helm.

Нам понадобятся следующие файлы со структурой каталогов:


```
.helm (здесь мы будем описывать деплой)
├── templates (объекты kubernetes в виде шаблонов)
│   ├── deployment.yaml (основное приложение)
│   ├── ingress.yaml (описание для ingress)
│   └── service.yaml (сервис для приложения)
├── secret-values.yaml (файл с секретными переменными)
└── values.yaml (файл с переменными для параметризации шаблонов)
```

Подробнее читайте в [нашей статье](https://habr.com/ru/company/flant/blog/423239/) из серии про Helm.

![alt_text](images/-4.png "image_tooltip")

#### Описание приложения в хельме

Для работы нашего приложения в среде Kubernetes понадобится описать сущности Deployment, Service, завернуть трафик на приложение, донастроив роутинг в кластере с помощью сущности Ingress. И не забыть создать отдельную сущность Secret, которая позволит нашему kubernetes пулить собранные образа из registry.

##### Запуск контейнера

Начнем с описания deployment.yaml

<details><summary>deployment.yaml</summary>
<p>

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Chart.Name }}-django
spec:
  revisionHistoryLimit: 3
  strategy:
    type: RollingUpdate
  replicas: 1
  selector:
    matchLabels:
      app: {{ .Chart.Name }}-django
  template:
    metadata:
      labels:
        app: {{ .Chart.Name }}-django
    spec:
      imagePullSecrets:
      - name: "registrysecret"
      containers:
      - name: {{ .Chart.Name }}-django
{{ tuple "django" . | include "werf_container_image" | indent 8 }}
        workingDir: /app
        command: ['gunicorn', 'tools.wsgi:application', '--bind', '0.0.0.0:80', '--access-logfile', '-', '--log-level', 'debug']
        ports:
        - containerPort: 80
          protocol: TCP
        env:
{{ tuple "django" . | include "werf_container_env" | indent 8 }}
```
</p>
</details>


Коснусь только шаблонизированных параметров. Значение остальных параметров можно найти в документации [Kubernetes](https://kubernetes.io/docs/concepts/).

`{{ .Chart.Name }}` - значение для данного параметра берётся из файла werf.yaml из поля **_project_**


werf.yaml:

```yaml
project: tools
configVersion: 1
```
Далее мы указываем имя сектрета в котором мы будем хранить данные для подключение к нашему registry, где хранятся наши образа.

```yaml
      imagePullSecrets:
      - name: "registrysecret"
```
О том как его создать мы опишем в конце главы.



Шаблон ниже отвечает за то чтобы вставить информацию касающуюся местонахождения нашего doсker image в registry, чтобы kubernetes знал откуда его скачать. А также политику пула этого образа.

```yaml
{{ tuple "django" . | include "werf_container_image" | indent 8 }}
```
 И в итоге эта строка будет заменена helm’ом на это:


```yaml
   image: registry.gitlab-example.com/tools/django:6e3af42b741da90f2bc674e5646a87ad6b81d14c531cc89ef4450585   
   imagePullPolicy: IfNotPresent
```

Замену производит сам werf из собственных переменных. Изменять эту конструкцию нужно только в двух местах:
1. Рядом в первой части “django”  -  это название вашего docker image, которые мы указывали в werf.yaml в поле **image**, когда описывали сборку.

2. Intent 8 - параметр указывает какое количество пробелов вставить перед блоком, делаем мы это чтобы не нарушить синтаксис yaml, где пробелы(отступы) играют важную разделительную роль.  \
При разработке особенно важно учитывать что yaml не воспринимает табуляцию **только пробелы**!

```yaml
{{ tuple "django" . | include "werf_container_env" | indent 8 }}
```
Одна из самых главных строк, отвечает непосредственно за то какую команду запустить при запуске приложения.

```yaml
        ports:
        - containerPort: 80
          protocol: TCP
```
Блок отвечающий за то какие порты необходимо сделать доступными снаружи контейнера, и по какому протоколу.

```yaml
{{ tuple "django" . | include "werf_container_env" | indent 8 }}
```
Этот шаблон позволяет werf работать с переменными.
Его назначение подробно описано в следующей главе.


Теперь, как мы и обещали перейдем к созданию сущности Secret, которая будет содержать доступы до images registry.
Вы можете использовать команду kubectl, из кластера или у себя на личном компьютере даже если он не имеет доступ к кластеру (он нам не понадобится).
Вы можете запустить следующую команду:
```bash
kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email> --dry-run=true -o yaml
```
В команде вы указываете данные пользователя для подключения и затем получаете такой вывод:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: registrysecret
  creationTimestamp: null
type: kubernetes.io/dockerconfigjson

data:
  .dockerconfigjson: eyJhdXRocyI6eyJyZWdpc3RyeS5leGFtcGxlLmNvbSI6eyJ1c2VybmFtZSI6InVzZXIiLCJwYXNzd29yZCI6InF3ZXJ0eSIsImVtYWlsIjoiZXhhbXBsZUBnbWFpbC5ydSIsImF1dGgiOiJkWE5sY2pweGQyVnlkSGs9In19fQ==

```
Команда сформировала готовый секрет и отдала его вам, зашифровав данные в base64. Сработало это благодаря флагам `--dry-run=true` и `-o yaml`, первый флаг говорит о том что мы хотим сымитировать создание сущности в кластере без доступа к нему, а второй о том что мы хотим видеть наши данные в формате `yaml`

Теперь вам осталось только создать отдельный файл Secret.yaml и положить в него содержимое которое выдала вам команда, предварительно удалив строку `creationTimestamp: null`.

P.S. Настоятельно не рекомендуем хранить данные подключения в сыром виде в котором нам выдала команда, о том каким образом можно зашифровать данные с помощью werf будет показано в главе [Секретные переменные](####секретные-переменные).

##### Переменные окружения

Для корректной работы нашего приложения ему нужно узнать переменные окружения.
По умолчанию Django запускает и без них, но это не позводит нам конфигурировать приложение на лету.
Поэтому в настройках мы изменим некоторые параметры, и будем брать их из переменных, например, `DEBUG` и `SECRET_KEY`.


И эти переменные можно параметризовать с помощью файла `values.yaml`.

Так например, мы пробросим значение переменной DEBUG в наш контейнер из `values.yaml`

```yaml
app:
  debug:
    stage: "1"
    production: "0"
```
И теперь добавляем переменную в наш Deployment.
```yaml
          - name: DEBUG
            value: {{ pluck .Values.global.env .Values.app.debug | first | default .Values.app.debug._default | quote }}
```
Конструкция указывает на то что в зависимости от значения .Values.global.env мы будем подставлять первое совпадающее значение из .Values.app.debug

Werf устанавливает значение .Values.global.env в зависимости от названия окружения указанного в .gitlab-ci.yml в стадии деплоя.

Теперь перейдем к описанию шаблона из предыдущей главы:


```yaml
        env:
{{ tuple "django" . | include "werf_container_env" | indent 8 }}
```
Werf закрывает ряд вопросов, связанных с перевыкатом контейнеров с помощью конструкции  [werf_container_env](https://ru.werf.io/documentation/reference/deploy_process/deploy_into_kubernetes.html#werf_container_env). Она возвращает блок с переменной окружения DOCKER_IMAGE_ID контейнера пода. Значение переменной будет установлено только если .Values.global.werf.is_branch=true, т.к. в этом случае Docker-образ для соответствующего имени и тега может быть обновлен, а имя и тег останутся неизменными. Значение переменной DOCKER_IMAGE_ID содержит новый ID Docker-образа, что вынуждает Kubernetes обновить объект.

Важно учесть что данный параметр не подставляет ничего при использовании стратегии тэгирования `stages-signature`, но мы настоятельно рекомендуем добавить его внутрь манифеста для удобства интеграции будущих обновлений werf.

Аналогично можно пробросить секретные переменные (пароли и т.п.) и у Верфи есть специальный механизм для этого. Но к этому вопросу мы вернёмся позже.


##### Логгирование

При запуске приложения в kubernetes необходимо логи отправлять в stdout и stderr - это необходимо для простого сбора логов например через `filebeat`, а так же чтобы не разростались docker образы запущенных приложений. По умолчанию Django не логирует ошибки и критичные логи, а отправляет на email администратора.

Мы предлагаем:
1. Писать все логи в stdout контейнера и чтобы оттуда их собирал сторонний сервис.

Чтобы все логи отправлялись в stdout, его нужно сконфигурировать в коде приложения. Добавьте в файл настроек словарь:

```yaml
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
        },
    },
    'root': {
        'handlers': ['console'],
        'level': 'WARNING',
    },
}
```

[Подбробно про логирование](https://docs.djangoproject.com/en/3.0/topics/logging/)

2. Ограничить их количество в stdout с помощью настройки для Docker в /etc/docker/daemon.json

```json
{
        "log-driver": "json-file",
        "log-opts": {
                "max-file": "5",
                "max-size": "10m"
        }
}
```
В общей сложности конструкция выше понятна, но если вы хотите разобрать её подробнее вы можете обратиться к официальной [документации](https://docs.docker.com/config/containers/logging/configure/).

##### Направление трафика на приложение

Нам надо будет пробить порт у пода, сервиса и настроить Ingress, который выступает у нас в качестве балансера.

Если вы мало работали с Kubernetes — эта часть может вызвать у вас много проблем. Большинство тех, кто начинает работать с Kubernetes по невнимательности допускают ошибки при конфигурировании labels и затем занимаются долгой и мучительной отладкой.

{{TODO: тут бы дать какую-то подсказку, как человеку пройти через это и не поседеть, если у него рядом нет ментора, который ткнёт ему в опечатку}}

###### Проброс портов

Для того чтобы мы смогли общаться с нашим приложением извне необходимо привязать к нашему deployment объект Service.

В наш service.yaml нужно добавить:

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: {{ .Chart.Name }}-django
spec:
  selector:
    app: {{ .Chart.Name }}-django
  clusterIP: None
  ports:
  - name: http
    port: 80
    protocol: TCP
```
Обязательно нужно указывать порты, на котором будет слушать наше приложение внутри контейнера. И в Service, как указано выше и в Deployment:

```yaml
        ports:
        - containerPort: 80
          protocol: TCP
```

Сама же привязка к deployment происходит с помощью блока **selector:**


```yaml
  selector:
    app: {{ .Chart.Name }}-django
```


Внутри селектора у нас указан лэйбл `app: {{ .Chart.Name }}-django` он должен полностью совпадать с блоком `labels` в Deployment который мы описывали в главах выше:



```yaml
  template:
    metadata:
      labels:
        app: {{ .Chart.Name }}-django
```


Иначе Kubernetes не поймет на какой именно под или совокупность подов Service указывать. Это важно еще и из-за того что ip адреса подов попадают в DNS Kubernetes под именем сервиса, что позволяет нам обращаться к поду с нашим приложения просто по имени сервиса.

Полная запись для пода в нашем случае будет выглядеть так:
`tools-django.stage.svc.cluster.local` и расшифровывается так - `имя_сервиса.имя_неймспейса.svc.cluster.local` - неизменная часть это стандартный корневой домен Kubernetes.

Интересно то что поды находящиеся внутри одного неймспейса могут обращаться друг к другу просто по имени сервиса.

Подробнее о том как работать с сервисами можно узнать в [документации](connect-applications-service).


###### Роутинг на Ingress
Теперь мы можем передать nginx ingress имя сервиса на который нужно проксировать запросы извне. 
<details><summary>ingress.yaml</summary>
<p>

```yaml
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
  name: {{ .Chart.Name }}-django
spec:
  rules:
  - host: {{ .Values.global.ci_url }}
    http:
      paths:
      - backend:
          serviceName: {{ .Chart.Name }}-django
          servicePort: 80
        path: /
```
</p>
</details>

Настройка роутинга происходит непосредственно в блоке `rules:`, где мы можем описать правила по которму трафик будет попадать в наше приложение.

`- host: {{ .Values.global.ci_url }}` - в данном поле мы описываем тот домен на который конечный пользователь будет обращаться чтобы попасть в наше приложение. Можно сказать что это точка входа в наше приложение.

`paths:` - отвечает за настройку путей внутри нашего домена. И принимает в себя список из конфигураций этих путей.
Далее мы прямо описываем что все запросы попадающие на корень `path: /`, мы отправляем на backend, которым выступает наш сервис:
```yaml
      - backend:
          serviceName: {{ .Chart.Name }}-django
          servicePort: 80
        path: /
```
Имя сервиса и его порт должны полностью совпадать с теми что мы описывали в сущности Service.
Удобство в том что описаний таких бэкендов может быть множество. И вы можете на одном домене по разным путям направлять трафик в разные приложения. Как это делать будет описано в последующих главах.
```

Обратите внимание на параметр {{ .Values.global.ci_url }}. Данный параметр передается из файла .gitlab-ci.yml

```yaml
.base_deploy:
  script:
    - werf deploy
      --set "global.ci_url=example.com"
```

Подобным образом, можно передавать и другие необходимые переменные.


#### Секретные переменные

Мы уже рассказывали о том как использовать обычные переменные в нашем СI забирая их напрямую из values.yaml. Суть работы с секретными переменными абсолютно та же, единственное что в репозитории они будут храниться в зашифрованном виде.

Потому для хранения в репозитории паролей, файлов сертификатов и т.п., рекомендуется использовать подсистему работы с секретами werf.

Идея заключается в том, что конфиденциальные данные должны храниться в репозитории вместе с приложением, и должны оставаться независимыми от какого-либо конкретного сервера.


Для этого в werf существует инструмент [helm secret](https://werf.io/documentation/reference/deploy_process/working_with_secrets.html). Чтобы воспользоваться шифрованием нам сначала нужно создать ключ, сделать это можно так: 

```bash
$ werf helm secret generate-secret-key
ad747845284fea7135dca84bde9cff8e
$ export WERF_SECRET_KEY=ad747845284fea7135dca84bde9cff8e
```

После того как мы сгенерировали ключ, добавим его в переменные окружения у себя локально.

Секретные данные мы можем добавить создав рядом с values.yaml файл secret-values.yaml

Теперь использовав команду:


```bash
$ werf helm secret values edit ./helm/secret-values.yaml
```


Откроется текстовый редактор по-умолчанию, где мы сможем добавить наши секретные данные как обычно:


```yaml
app:
  s3:
    access_key:
      _default: bNGXXCF1GF
    secret_key:
      _default: zpThy4kGeqMNSuF2gyw48cOKJMvZqtrTswAQ
```


После того как вы закроете редактор, werf зашифрует их и secret-values.yaml будет выглядеть так:

И вы сможете добавить их в переменные окружения в Deployment точно так же как делали это с обычными переменными. Главное это не забыть добавить ваш WERF_SECRET_KEY в переменные репозитория гитлаба, найти их можно тут Settings -> CI/CD -> Variables. Настройки репозитория доступны только участникам репозитория с ролью выше Administrator, потому никто кроме доверенных лиц не сможет получить наш ключ. А werf при деплое нашего приложения сможет спокойно получить ключ для расшифровки наших переменных.

#### Деплой в Gitlab CI

Теперь мы наконец приступаем к описанию стадии выката. Потому продолжаем нашу работу в gitlab-ci.yml.

Мы уже решили, что у нас будет два окружения, потому под каждое из них мы должны описать свою стадию, но в общей сложности они будут отличаться только параметрами, потому мы напишем для них небольшой шаблон:

```yaml
.base_deploy: &base_deploy
  script:
    - werf deploy --stages-storage :local 
      --set "global.ci_url=$(cut -d / -f 3 <<< $CI_ENVIRONMENT_URL)"
  dependencies:
    - Build
  tags:
    - werf
```

Скрипт стадий выката отличается от сборки всего одной командой:

```yaml
    - werf deploy --stages-storage :local
      --set "global.ci_url=$(cut -d / -f 3 <<< $CI_ENVIRONMENT_URL)"
```

И тут назревает вполне логичный вопрос.

Как werf понимает куда нужно будет деплоить и каким образом? На это есть два ответа.

Первый из них вы уже видели и заключается он в команде `werf ci-env` которая берёт нужные переменные прямиком из pipeline Gitlab - и в данном случае ту что касается названия окружения.

А второй это описание стадий выката в нашем gitlab-ci.yml:

```yaml
Deploy to Stage:
  extends: .base_deploy
  stage: deploy
  environment:
    name: stage
    url: https://stage.example.com
  only:
    - merge_requests
  when: manual

Deploy to Production:
  extends: .base_deploy
  stage: deploy
  environment:
    name: production
    url: http://example.com
  only:
    - master
```

Описание деплоя содержит в себе немного. Скрипт, указание принадлежности к стадии **deploy**, которую мы описывали в начале gitlab-ci.yml, и **dependencies** что означает что стадия не может быть запущена без успешного завершения стадии **Build**. Также мы указали с помощью **only**, ветку _master_, что означает что стадия будет доступна только из этой ветки. **environment** указали потому что werf необходимо понимать в каком окружении он работает. В дальнейшем мы покажем, как создать CI для нескольких окружений. Остальные параметры вам уже известны.

И что не мало важно **url** указанный прямо в стадии. 
1. Это добавляет в MR и pipeline дополнительную кнопку по которой мы можем сразу попасть в наше приложение. Что добавляет удобства.
2. С помощью конструкции `--set "global.ci_url=$(cut -d / -f 3 <<< $CI_ENVIRONMENT_URL)"` мы добавляем адрес в глобальные переменные проекта и затем можем например использовать его динамически в качестве главного домена в нашей сущности Ingress:
```yaml
      - host: {{ .Values.global.ci_url }}
```
По умолчанию деплой будет происходить в namespace состоящий из имени проекта задаваемого в `werf.yaml` и имени окружения задаваемого в `.gitlab-ci.yml` куда мы деплоим наше приложение.

Ну а теперь достаточно создать Merge Request и нам будет доступна кнопка Deploy to Stage.

![alt_text](images/-6.png "image_tooltip")

Посмотреть статус выполнения pipeline можно в интерфейсе gitlab **CI / CD - Pipelines**

![alt_text](images/-7.png "image_tooltip")


Список всех окружений - доступен в меню **Operations - Environments**

![alt_text](images/-8.png "image_tooltip")

Из этого меню - можно так же быстро открыть приложение в браузере.

{{И тут в итоге должна быть картинка как аппка задеплоилась и объяснение картинки}}

# Подключаем зависимости

Werf подразумевает, что лучшей практикой будет разделить сборочный процесс на этапы, каждый с четкими функциями и своим назначением. Каждый такой этап соответствует промежуточному образу, подобно слоям в Docker. В werf такой этап называется стадией, и конечный образ в итоге состоит из набора собранных стадий. Все стадии хранятся в хранилище стадий, которое можно рассматривать как кэш сборки приложения, хотя по сути это скорее часть контекста сборки.

Стадии — это этапы сборочного процесса, кирпичи, из которых в итоге собирается конечный образ. Стадия собирается из группы сборочных инструкций, указанных в конфигурации. Причем группировка этих инструкций не случайна, имеет определенную логику и учитывает условия и правила сборки. С каждой стадией связан конкретный Docker-образ. Подробнее о том, какие стадии для чего предполагаются можно посмотреть в [документации](https://ru.werf.io/documentation/reference/stages_and_images.html).

Werf предлагает использовать для стадий следующую стратегию:

*   использовать стадию beforeInstall для инсталляции системных пакетов;
*   использовать стадию install для инсталляции системных зависимостей и зависимостей приложения;
*   использовать стадию beforeSetup для настройки системных параметров и установки приложения;
*   использовать стадию setup для настройки приложения.

Подробно про стадии описано в [документации](https://ru.werf.io/documentation/configuration/stapel_image/assembly_instructions.html).

Одно из основных преимуществ использования стадий в том, что мы можем не перезапускать нашу сборку с нуля, а перезапускать её только с той стадии, которая зависит от изменений в определенных файлах.

В нашем случае в качестве примера мы можем взять файл `requirements.txt`.

Те кто уже сталкивался с разработкой на python приложений знают, что в файле `requirements.txt` указываются зависимости которые нужны для сборки приложения. Потому самое логичное указать данный файл в зависимости сборки, чтобы в случае изменений в нём, была перезапущена сборка только со стадии **_install_**.

Для этого в одной из первых глав мы сразу и добавляли наш файл requirements.txt в зависимости werf.
В Django в качестве менеджера зависимостей используется pip. Пропишем его использование в файле `werf.yaml` и затем оптимизируем его использование.

## Подключение менеджера зависимостей

Пропишем команды `pip install` в нужные стадии сборки в `werf.yaml`

```yaml
  install:
  - name: Install python requirements
    pip:
      requirements: /usr/src/app/requirements.txt
      executable: pip3.6

```

Однако, если оставить всё так — стадия `install` не будет запускаться при изменении файла со списком пакетов. Подобная зависимость пользовательской стадии от изменений [указывается с помощью параметра git.stageDependencies](https://ru.werf.io/documentation/configuration/stapel_image/assembly_instructions.html#%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D0%BC%D0%BE%D1%81%D1%82%D1%8C-%D0%BE%D1%82-%D0%B8%D0%B7%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9-%D0%B2-git-%D1%80%D0%B5%D0%BF%D0%BE%D0%B7%D0%B8%D1%82%D0%BE%D1%80%D0%B8%D0%B8):

```yaml
git:
- add: /
  to: /app
  stageDependencies:
    install:
    - requirements.txt
```

При изменении файла `requirements.txt` стадия `install` будет запущена заново.

## Оптимизация сборки

У pip есть кеш, чтобы каждый раз менеджер зависимостей не скачивал заново один и тот-же пакет.
Находится он в папке: `~/.cache/pip/`

Для того, чтобы оптимизировать работу с этим кешом при сборке, мы добавим специальную конструкцию в werf.yaml:

```yaml
mount:
- from: build_dir
  to: /app/.cache/pip
```

При каждом запуске билда, эта директория будет мантироваться с сервера, где запускается билд, и  не будет очищаться между билдами.
Так между сборками, у нас сохранится этот кеш.

# Генерируем и раздаем ассеты

В какой-то момент в процессе разработки вам понадобятся ассеты (т.е. картинки, css, js).

Для генерации ассетов мы будем использовать команду `python manage.py collectstatic --noinput`.

Интуитивно понятно, что на стадии сборки нам надо будет вызвать скрипт, который генерирует файлы, т.е. что-то надо будет дописать в `werf.yaml`. Однако, не только там — ведь какое-то приложение в production должно непосредственно отдавать статические файлы. Мы не будем отдавать файлики с помощью Django. Хочется, чтобы статику раздавал nginx. А значит надо будет внести какие-то изменения и в helm чарты.

## Сценарий сборки ассетов

Команда `python manage.py collectstatic --noinput` прекомпилирует файлы в директорию, которая задается параметром `STATIC_ROOT`, практически в любом проекте она одинакова, и в итоге файлы сгенерируются в папке `<PROJECT_DIR>/static`.

Тут есть один нюанс - при сборке приложения мы не рекомендуем использовать какие-либо изменяемые переменные. Потому что собранный бинарный образ должен быть независимым от конкретного окружения. А значит во время сборки у нас не может быть, например, базы данных, user-generated контента и подобных вещей.


## Какие изменения необходимо внести

Генерация ассетов происходит в артефакте на стадии `setup`, так как данная стадия рекомендуется для настройки приложения


Для уменьшения нагрузки на процесс основного приложения которое обрабатыаем логику работы rails приложения мы будем отдавать статические файлы через `nginx`
Мы запустим оба контейнера одним деплойментом и все запросы будет приходить вначале на nginx и если в запросе не будет отдача статических файлов - запрос будет отправлен прмложению.

### Изменения в сборке


Добавим стадию сборки ассетов для приложения в файл `werf.yaml`

```
  setup:
  - name: Install python requirements
    pip:
      requirements: /usr/src/app/requirements.txt
      executable: pip3.6

```

Окей, а в каком контейнере в конечном итоге должны оказаться собранные файлы? Есть минимум два варианта:

*   Делать один образ в котором: django, сгенерированные ассеты, nginx. Запускать этот один и тот же образ двумя разными способами (с разным исполняемым файлом)
*   Делать два образа: django отдельно, nginx + сгенерированные ассеты отдельно.

В первом варианте при каждом изменении будут перекатываться оба контейнера. Такое себе в большинстве случаев.

Пойдём вторым путём.


В образе с нашим приложением мы не хотим чтобы у нас была установлена среда для сборки приложения и nginx а также для того чтобы уменьшить размеры образов - мы воспользуемся сборкой с помощью артефактов.

[Артефакт](https://ru.werf.io/documentation/configuration/stapel_artifact.html) — это специальный образ, используемый в других артефактах или отдельных образах, описанных в конфигурации. Артефакт предназначен преимущественно для отделения ресурсов инструментов сборки от процесса сборки образа приложения. Примерами таких ресурсов могут быть — программное обеспечение или данные, которые необходимы для сборки, но не нужны для запуска приложения, и т.п.

С помощью такого подхода мы сможем собрать и подготовить все файлы и зависимости в одном образе и импортировать нужные нам файлы по двум разным docker контейнерам, где в одном - будет среда для выполнения приложения django а во втором - только nginx со статическими файлами.

Разница `artifact` и `image` так же состоит в том - что `artifact` нельза запустиль локально для дебага как `image` командой `werf run`

Импорт указывается отдельной директивой следующим в следующем синтаксисе:

```yaml
---
artifact: build
fromImage: django
ansible:
  install:
  - name: "mkdir"
    file:
      path: /usr/src/app/static
      state: directory
  - name: "Build static"
    command: "python manage.py collectstatic --noinput"
    args:
      chdir: /usr/src/app
---
image: static
from: nginx:1.17-alpine
ansible:
  setup:
  - name: "Add nginx config"
    copy:
      content: |
{{ .Files.Get ".werf/nginx.conf" | indent 8 }}
      dest: /etc/nginx/nginx.conf
import:
- artifact: build
  add: /usr/src/app/collect_static
  to: /app/static
  after: setup
```


Подготовленные ассеты мы будет отдавать через отдельный nginx контейнер в поде чтобы не загружать основное приложение лишними подключениями. Для этого так-же производится импорт подготовленных файлов в отдельный образ.
При работе мы планируем, что все запросы будут проксироваться через nginx, поэтому заменяем файл `/etc/nginx/nginx.conf` на необходимый нам, который находится также в репозитории с приложением. Такой подход позволит нам управлять лимитом подключений который может принять приложение.


### Изменения в деплое


При таком подходе изменим деплой нашего приложения добавив еще один деплоймент с статикой.  
Укажем livenessProbe и readinessProbe, которые будут проверять корректную работу контейнера в поде. preStop команда необходима для корректного завершение процесса nginx. В таком случае при новом выкате новой версии приложения будет корректное завершение всех активных сессий.

```yaml
      - name: assets
{{ tuple "statuc" . | include "werf_container_image" | indent 8 }}
        lifecycle:
          preStop:
            exec:
              command: ["/usr/sbin/nginx", "-s", "quit"]
        livenessProbe:
          httpGet:
            path: /healthz
            port: 80
            scheme: HTTP
        readinessProbe:
          httpGet:
            path: /healthz
            port: 80
            scheme: HTTP
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
```

В описании сервиса - так же должен быть указан правильный порт

```yaml
  ports:
  - name: http
    port: 80
    protocol: TCP
```
### Изменения в роутинге


Поскольку у нас маршрутизация запросов происходит черех nginx контейнер а не на основе ingress ресурсов - нам необходимо только указать коректный порт для сервиса

```yaml
      paths:
      - path: /
        backend:
          serviceName: {{ .Chart.Name }}-django
          servicePort: 80
```

Если мы хотим разделять трафик на уровне ingress - нужно разделить запросы по path и портам

```yaml
      paths:
      - path: /
        backend:
          serviceName: {{ .Chart.Name }}-django
          servicePort: 80
      - path: /static
        backend:
          serviceName: {{ .Chart.Name }}-static
          servicePort: 80
```
# Работа с файлами

В разработке может встретиться возможность когда требуется сохранять загружаемые пользователями файлы. Встает резонный вопрос о том каким образом их нужно хранить, и как после этого получать.

Первый и более общий способ. Это использовать как volume в подах [NFS](https://kubernetes.io/docs/concepts/storage/volumes/#nfs), [CephFS](https://kubernetes.io/docs/concepts/storage/volumes/#cephfs) или [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath), который будет направлен на директорию на ноде, куда будет подключено одно из сетевых хранилищ.

Мы не рекомендуем этот способ, потому что при возникновении неполадок с такими типами volume’ов мы будем влиять на работоспособность всего докера, контейнера и демона docker в целом, тем самым могут пострадать приложения которые даже не имеют отношения к вашему приложению.

Мы рекомендуем пользоваться S3. Такой способ выглядит намного надежнее засчет того что мы используем отдельный сервис, который имеет свойства масштабироваться, работать в HA режиме, и будет иметь высокую доступность.

Есть cloud решения S3, такие как AWS S3, Google Cloud Storage, Microsoft Blobs Storage и т.д. которые будут самым надежным решением из всех что мы можем использовать.






Данная настройка производится полностью в рамках приложения а нам остается только передать необходимые переменные окружения при запуске приложения.


Если мы будем сохранять файлы какой - либо директории у приложения запущенного в kubernetes - то после перезапуска контейнера все изменения пропадут.

# Работа с электронной почтой

Работа с электронной почтой производится с помощью внешнего api например mailgun.
В django есть отдельный пакет для работы с ним.
Устанавливаем его: `pip install django-mailgun`

Далее, в настройках необходимо указать параметры подключения к MAILGUN, и после им можно пользоваться:
```yaml
EMAIL_BACKEND = 'django_mailgun.MailgunBackend'
MAILGUN_ACCESS_KEY = 'ACCESS-KEY'
MAILGUN_SERVER_NAME = 'SERVER-NAME'
```
Более подробно все описано в оффициальной документации пакета - https://pypi.org/project/django-mailgun/



Главное заметить, что мы также как и в остальных случаях выносим основную конфигурацию в переменные. А далее мы по тому же принципу добавляем их параметризированно в наш secret-values.yaml не забыв использовать [шифрование](####секретные-переменные).
```yaml
  mailgun_apikey:
    _default: 192edaae18f13aaf120a66a4fefd5c4d-7fsaaa4e-kk5d08a5
  mailgun_domain:
    _default: sandboxf1b90123966447a0514easd0ea421rba.mailgun.org
```
И теперь мы можем использовать их внутри манифеста.
```yaml
        - name: MAILGUN_APIKEY
          value: {{ pluck .Values.global.env .Values.app.mailgun_apikey | first | default .Values.app.mailgun_apikey._default }}
        - name: MAILGUN_SERVER_NAME
          value: {{ pluck .Values.global.env .Values.app.mailgun_domain | first | default .Values.app.mailgun_domain._default | quote }}
```


# Подключаем redis

Допустим к нашему приложению нужно подключить простейшую базу данных, например, redis или memcached. Возьмем первый вариант.

В простейшем случае нет необходимости вносить изменения в сборку — всё уже собрано для нас. Надо просто подключить нужный образ, а потом в вашем django приложении корректно обратиться к этому приложению.

Все данные для подключения у нас будут передаваться через переменные окружения.

## Завести Redis в Kubernetes

Есть два способа подключить: прописать helm-чарт самостоятельно или подключить внешний. Мы рассмотрим второй вариант.

Подключим redis как внешний subchart.

Для этого нужно:

1. прописать изменения в yaml файлы;
2. указать редису конфиги
3. подсказать werf, что ему нужно подтягивать subchart.

Добавим в файл `.helm/requirements.yaml` следующие изменения:

```yaml
dependencies:
- name: redis
  version: 9.3.2
  repository: https://kubernetes-charts.storage.googleapis.com/
  condition: redis.enabled
```

Для того чтобы werf при деплое загрузил необходимые нам сабчарты - нужно добавить команды в `.gitlab-ci`

```yaml
.base_deploy:
  stage: deploy
  script:
    - werf helm repo init
    - werf helm dependency update
    - werf deploy
```

Опишем параметры для redis в файле `.helm/values.yaml`

```yaml
redis:
  enabled: true
```

При использовании сабчарта по умолчанию создается master-slave кластер redis.

Если посмотреть на рендер (`werf helm render`) нашего приложения с включенным сабчартом для redis, то можем увидеть какие будут созданы сервисы:

```yaml
# Source: example-2/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-master

# Source: example-2/charts/redis/templates/redis-slave-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: example-2-stage-redis-slave
```

## Подключение Django приложения к базе redis

В нашем приложении - мы будем  подключаться к мастер узлу редиса. Нам нужно, чтобы при выкате в любое окружение приложение подключалось к правильному редису.

Рассмотрим настройки подключения к redis из нашего приложения.

```yaml
CACHES = {
    "default": {
        "BACKEND": "django_redis.cache.RedisCache",
        "LOCATION": os.environ.get('REDIS_URL', default="redis://127.0.0.1:6379/1"),
        "OPTIONS": {
            "CLIENT_CLASS": "django_redis.client.DefaultClient"
        },
        "KEY_PREFIX": os.environ.get('REDIS_KEY_PREFIX', default="example")
    }
}
```

В данном файле мы видим что адрес подключения берется из переменной окружения `REDIS_URL` и если такая переменная не задана - подставляется значение по умолчанию `redis://127.0.0.1:6379/1`


Чтобы добавить redis в качестве кеша, нам необходимо установить пакет ``, и добавить в файл настроек `settings.py` следующие строки:

Для подключения нашего приложения к redis нам необходимо установить дополнительный пакет `pip install django-redis` и указать переменную окружения `REDIS_URL` при деплое нашего приложения в файле с описанием деплоймента.


```
- name: REDIS_URL
  value: "redis://{{ .Chart.Name }}-{{ .Values.global.env }}-redis-master:6379/1"
```


В итоге, при деплое нашего приложения преобразуется например в строку

`redis://example-2-stage-redis-master:6379/1` для stage окружения

Более подробно про подключение к редису можно почитать в [документации](https://github.com/jazzband/django-redis)


# Подключаем базу данных

Для текущего примера в приложении должны быть установлены необходимые зависимости. В качестве примера - мы возьмем приложение для работы которого необходима база данных.


## Как подключить БД


Подключим postgresql helm сабчартом, для этого внесем изменения в файл `.helm/requirements.yaml`


```
dependencies:
- name: postgresql
  version: 8.0.0
  repository: https://kubernetes-charts.storage.googleapis.com/
  condition: postgresql.enabled
```


Для того чтобы werf при деплое загрузил необходимые нам сабчарты - нужно добавить команды в .gitlab-ci


```
.base_deploy:
  stage: deploy
  script:
    - werf helm repo init
    - werf helm dependency update
    - werf deploy
```


Опишем параметры для postgresql в файле `.helm/values.yaml`


```
postgresql:
  enabled: true
  postgresqlDatabase: hello_world
  postgresqlUsername: hello_world_user
  postgresqlHost: postgres
  imageTag: "12"
  persistence:
    enabled: true
```


## Подключение Django приложения к базе postgresql

Для того, чтобы django научился взаимодействовать с postgresql, нужно поставить дополнительный пакет `pip install psycopg2-binary`.

Настройки подключения нашего приложения к базе данных мы будем передавать через переменные окружения. Такой подход позволит нам использовать один и тот же образ в разных окружениях, что должно исключить запуск непроверенного кода в production окружении.

Внесем изменения в файл настроек `settings.py`.


```
DATABASES = {
    'default': {
        'ENGINE': os.environ.get('SQL_ENGINE', 'django.db.backends.sqlite3'),
        'NAME': os.environ.get('SQL_DATABASE', os.path.join(BASE_DIR, 'db.sqlite3')),
        'USER': os.environ.get('SQL_USER', 'user'),
        'PASSWORD': os.environ.get('SQL_PASSWORD', 'password'),
        'HOST': os.environ.get('SQL_HOST', 'localhost'),
        'PORT': os.environ.get('SQL_PORT', '5432'),
    }
}
```


Параметры подключения приложения к базе данным мы опишем в файле `.helm/templates/_envs.tpl`


```
{{- define "database_envs" }}
- name: SQL_HOST
  value: "postgres://{{ .Values.postgresql.postgresqlUsername }}:{{ .Values.postgresql.postgresqlPassword }}@{{ .Chart.Name }}-{{ .Values.global.env }}-postgresql:5432"
- name: SQL_DATABASE
  value: {{ .Values.postgresql.postgresqlDatabase }}
- name: SQL_USER
  value: {{ .Values.postgresql.postgresqlUser }}
- name: SQL_PASSWORD
  value: {{ .Values.postgresql.postgresqlPassword }}
- name: SQL_PORT
  value: {{ .Values.postgresql.postgresqlPort }}
- name: SQL_ENGINE
  value: "django.db.backends.postgresql_psycopg2"
{{- end }}
```


Такой подход позволит нам переиспользовать данное определение переменных окружения для нескольких контейнеров. Имя для сервиса postgresql генерируется из названия нашего приложения, имени окружения и добавлением postgresql

Остальные значения подставляются из файлов values.yaml и secret-values.yaml


Пароль от базы данных добавим в `secret-values.yaml`

## Выполнение миграций


Запуск миграций производится созданием приметива Job в kubernetes. Это единоразовый запуск пода с необходимыми нам контейнерами.

Добавим запуск миграций после каждого деплоя приложения.


```yaml
---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Chart.Name }}-migrations
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/weight": "2"
spec:
  activeDeadlineSeconds: 600
  template:
    metadata:
      name: {{ .Chart.Name }}-migrate
    spec:
      restartPolicy: Never
      initContainers:
      - name: wait-postgres
        command: ['/bin/sh', '-c', 'while ! getent ahostsv4 {{ pluck .Values.global.env .Values.db.host | first | default .Values.db.host._default }}; do sleep 1; done']
        image: alpine:3.6
        env:
      containers:
      - name: migration
        args: ["python manage.py migrate --noinput"]
        command:
        - /bin/bash
        - -c
        - --
        workingDir: "/usr/src/app"
{{ tuple "django" . | include "werf_container_image" | indent 8 }}
        env:
{{- include "env_app" . | indent 8 }}
{{ tuple "django" . | include "werf_container_env" | indent 8 }}
```


Аннотации `"helm.sh/hook": post-install,post-upgrade` указывают условия запуска job а `"helm.sh/hook-weight": "2"` указывают на порядок выполнения (от меньшего к большему)

При запуске миграций мы используем тот же самый образ что и в деплойменте. Различие только в запускаемых командах.


# Юнит-тесты и Линтеры


Запуск тестов и линтеров - это отдельные стадии в pipelinе для выполнения которых могут быть нужны определенные условия.

В django есть встроенный механизм для их запуска, это стандартный `unittest`. Что-бы ими воспользоваться, необходимо  собрать образ приложения и запустить выполнение задания отдельной стадией на нашем gitlab runner командной [werf run](https://ru.werf.io/documentation/cli/main/run.html).


```
Unittests:
  script:
    - werf run django -- python manage.py test
```


При таком запуске наш kubernetes кластер не задействован.

Если нам нужно проверить приложение линтером, но данные зависимости не нужны в итоговом образе - нам необходимо собрать отдельный образ. Данный пример будет в репозитории с примерами а тут мы его не будем описывать.


# Несколько приложений в одной репе

Если в одном репозитории находятся несколько приложений например для backend и frontend необходимо использовать сборку приложения с несколькими образами.

Мы рассказывали [https://www.youtube.com/watch?v=g9cgppj0gKQ](https://www.youtube.com/watch?v=g9cgppj0gKQ) о том, почему и в каких ситуациях это — хороший путь для микросервисов.

Покажем это на примере, запустим рядом с нашим приложением простой websocket chat.

## Сборка приложений

Сборка приложения с несколькими образами описана в [статье](https://ru.werf.io/documentation/guides/advanced_build/multi_images.html). На ее основе покажем наш пример для нашего приложения.

Структура каталогов будет организована следующим образом


```
├── .helm
│   ├── templates
│   └── values.yaml
├── django
├── chat
└── werf.yaml
```



Сборка для chat приложения описана в файле werf.yaml как отдельный образ



```yaml
image: chat
from: node:10.16.3
git:
- add: /chat
  to: /app
ansible:
  beforeInstall:
  - name: install dependencies
    apt:
      name:
      - yarn
  install:
  - name: install node dependencies
    shell: yarn install
    args:
      chdir: /app
  setup:
  - name: build js app
    shell: yarn build
    args:
      chdir: /app
```


{{1. Добавляем кронджоб}}
{{2. Добавляем воркер/консюмер}}
{{3. Добавляем вторую приложуху на другом языке (например, это может быть webscoket’ы на nodejs; показать организацию helm, организацию werf.yaml, и ссылку на другую статью)}}

# Динамические окружения

Не редко необходимо разрабатывать и тестировать сразу несколько feature для вашего приложения, и нет понимания как это делать, если у вас всего два окружения. Разработчику или тестеру приходится дожидаться своей очереди на контуре stage и затем проводить необходимые манипуляции с кодом (тестирование, отладка, демонстрация функционала). Таким образом разработка сильно замедляется. 

Решением этой проблемы мы предлагаем использовать динамические окружения. Их суть в том что мы можем развернуть и погасить такие окружения в любой момент, тем самым разработчик может проверить работает ли его код развернув его в динамическое окружение, после убедившись, он может его погасить до тех пор пока его feature не будет смерджена в общий контур или пока не придет тестер, который сможет развернуть окружение уже для своих нужд.

Рассмотрим примеры того что мы должны добавить в наш .gitlab-ci.yml, чтобы наши динамические окружения заработали:


```yaml
Deploy to Review:
  extends: .base_deploy
  stage: deploy
  environment:
    name: review/${CI_COMMIT_REF_SLUG}
    url: http://${CI_COMMIT_REF_SLUG}.k8s.example.com
    on_stop: Stop Review
  only:
    - feature/*
  when: manual
```
На первый взгляд стадия не отличается ничем от тех что мы описывали ранее, но мы добавили зависимость `on_stop: Stop Review` простыми словами означающую, что мы будем останавливать наше окружение следующей стадией:

```yaml
Stop Review:
  stage: deploy
  variables:
    GIT_STRATEGY: none
  script:
    - werf dismiss --env $CI_ENVIRONMENT_SLUG --namespace ${CI_ENVIRONMENT_SLUG} --with-namespace
  when: manual
  environment:
    name: review/${CI_COMMIT_REF_SLUG}
    action: stop
  only:
    - feature/*
```
`GIT_STRATEGY: none` - говорит нашему ранеру, что check out нашего кода не требуется.

`werf dismiss` - отвечает за то чтобы удалить из кластера helm релиз с нашим приложением где `$CI_ENVIRONMENT_SLUG` это переменна описывающая название нашего окружения, но как мы видим что в названии нашего окружение имеется символ `/`, `_SLUG` переменные в Gitlab отвечают за то что заменяют все невалидные символы оригинальных переменных на `-`, тем самым позволяя нам избежать проблем с их использованием, особенно в kubernetes (т.к. символ `/` запрещен в названиях любых сущностей)

Вопрос в том зачем мы вообще использовали такой символ внутри названия окружения, дело в том что Gitlab может распределять свои окружения в директории, и тем самым мы отделили все динамические окружения и определили их в директорию `review`

Подробнее о том как описываются динамические окружения в CI можно найти [тут](https://docs.gitlab.com/ee/ci/yaml/#environmentaction)

При таком ci - мы можем выкатывать каждую ветку `feature/*` в отдельный namespace с изолированной базой данных, накатом необходимых миграций и например проводить тесты для данного окружения.

В репозитории с примерами будет реализовано отдельное приложение которое показывает реализацию данного подхода.
